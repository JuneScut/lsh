{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: -Dspark.history.fs.logDirectory\n",
      "Warning: Ignoring non-Spark config property: export\n",
      "Warning: Ignoring non-Spark config property: -Dspark.history.ui.port\n",
      "Warning: Ignoring non-Spark config property: -Dspark.history.retainedApplications\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 09:19:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/20 09:19:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/04/20 09:19:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/04/20 09:19:44 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/04/20 09:19:44 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/04/20 09:19:44 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "\n",
    "import random\n",
    "\n",
    "config = SparkConf().setAppName(\"LSH\").setMaster(\"local[8]\")\n",
    "sc = SparkContext.getOrCreate(config)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD 形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SHINGLE_LENGTH = 3\n",
    "PRIME = 2147483647\n",
    "SIMILARITY_THRESHOLD = 0.001\n",
    "BAND_SIMILARITY_THRESHOLD = 0.001\n",
    "NUMBER_OF_BANDS = 10\n",
    "NUMBER_OF_ROWS = 5\n",
    "NUMBER_OF_HASHFUNCTIONS = 10\n",
    "\n",
    "\n",
    "def preprocessDocument(document):\n",
    "    return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \")\n",
    "\n",
    "def shingle(document):\n",
    "  resultingList = []\n",
    "  i = 0\n",
    "  while i + SHINGLE_LENGTH < len(document):\n",
    "    resultingList.append(hash(document[i:i+SHINGLE_LENGTH]))\n",
    "    i += 1\n",
    "  return resultingList\n",
    "\n",
    "def minHash(listOfShingles, hashFunctions):\n",
    "  result = []\n",
    "  for ind in range(len(hashFunctions)):\n",
    "    minVal = float(\"inf\")\n",
    "    for shingle in listOfShingles:\n",
    "      hashResult = hashFunctions[ind](shingle)\n",
    "      if hashResult < minVal:\n",
    "        minVal = hashResult\n",
    "    result.append(minVal)\n",
    "  return result\n",
    "\n",
    "def getHashFunctions(number):\n",
    "  result = []\n",
    "  rand = random.Random()\n",
    "  for i in range(number):\n",
    "    a = rand.randint(0, PRIME-1)\n",
    "    b = rand.randint(0, PRIME-1)\n",
    "    result.append(lambda x, a=a, b=b: ((a * x + b) % PRIME))\n",
    "  return result\n",
    "\n",
    "def signatureToHashedBandsOfRows(signature, numberOfBands, numberOfRowsInBand):\n",
    "  if len(signature) != numberOfBands * numberOfRowsInBand:\n",
    "    raise Exception(\"Wrong arguments number of bands times number of rows should equal length of signature\")\n",
    "  i = 0\n",
    "  bands = []\n",
    "  while i + numberOfRowsInBand <= len(signature):\n",
    "    bands.append(signature[i:i+numberOfRowsInBand])\n",
    "    i += numberOfRowsInBand\n",
    "  return [hash(tuple(band)) for band in bands]\n",
    "\n",
    "def findSimilarity(ints, ints1):\n",
    "  intersection = set(ints).intersection(set(ints1))\n",
    "  union = set(ints).union(set(ints1))\n",
    "  return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "| _1| _2|                 _3|\n",
      "+---+---+-------------------+\n",
      "|  1|  0| 0.8181818181818182|\n",
      "|  3|  0|0.05263157894736842|\n",
      "|  3|  1|0.05263157894736842|\n",
      "|  3|  2|0.05263157894736842|\n",
      "+---+---+-------------------+\n",
      "\n",
      "+---+---+-------------------+\n",
      "| _1| _2|                 _3|\n",
      "+---+---+-------------------+\n",
      "|  1|  0| 0.8181818181818182|\n",
      "|  3|  0|0.05263157894736842|\n",
      "|  3|  1|0.05263157894736842|\n",
      "|  3|  2|0.05263157894736842|\n",
      "+---+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import udf,col\n",
    "\n",
    "# sharedHashFunctions = sc.broadcast(getHashFunctions(NUMBER_OF_HASHFUNCTIONS))\n",
    "sharedHashFunctions = getHashFunctions(NUMBER_OF_HASHFUNCTIONS)\n",
    "\n",
    "textFile = sc.textFile(\"../dataset/text.txt\")\n",
    "documents = textFile.zipWithIndex().map(lambda x: (x[1], preprocessDocument(x[0])))\n",
    "shingled = documents.map(lambda x: (x[0], shingle(x[1])))\n",
    "minHashed = shingled.map(lambda x: (x[0], minHash(x[1], sharedHashFunctions)))\n",
    "\n",
    "similarities = minHashed.cartesian(minHashed) \\\n",
    "  .filter(lambda x: x[0][0] > x[1][0]) \\\n",
    "  .map(lambda x: (x[0][0], x[1][0], findSimilarity(x[0][1], x[1][1]))) \\\n",
    "  .filter(lambda x: x[2] > SIMILARITY_THRESHOLD)\n",
    "similarities.toDF().show()\n",
    "# print(similarities.collect())\n",
    "\n",
    "bands = minHashed.map(lambda x: (x[0], signatureToHashedBandsOfRows(x[1], NUMBER_OF_BANDS, NUMBER_OF_ROWS)))\n",
    "bands.cache()\n",
    "similarities.toDF().show()\n",
    "\n",
    "\n",
    "# lsh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=NUMBER_OF_BANDS)\n",
    "\n",
    "# data = [(Vectors.dense(band), [i]) for (i, band) in bands.collect()]\n",
    "# df = spark.createDataFrame(data, [\"features\", \"id\"])\n",
    "# model = lsh.fit(df)\n",
    "# model.transform(df).show()\n",
    "\n",
    "# result = model.approxSimilarityJoin(df, df, BAND_SIMILARITY_THRESHOLD, distCol=\"JaccardDistance\") \n",
    "# result = result.withColumn(\"Similariy\", 1-col(\"JaccardDistance\"))\n",
    "#   # .filter(\"JaccardDistance > \" + str(BAND_SIMILARITY_THRESHOLD)) \\\n",
    "  \n",
    "\n",
    "# result.select(\"datasetA.id\", \"datasetB.id\", \"JaccardDistance\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe 形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SHINGLE_LENGTH = 3\n",
    "PRIME = 2147483647\n",
    "SIMILARITY_THRESHOLD = 0.001\n",
    "BAND_SIMILARITY_THRESHOLD = 0.001\n",
    "NUMBER_OF_BANDS = 10\n",
    "NUMBER_OF_ROWS = 5\n",
    "NUMBER_OF_HASHFUNCTIONS = 20\n",
    "\n",
    "\n",
    "def preprocessDocument(document):\n",
    "    return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \") if isinstance(document, str) else document\n",
    "\n",
    "def shingle(document):\n",
    "  resultingList = []\n",
    "  i = 0\n",
    "  while i + SHINGLE_LENGTH < len(document):\n",
    "    resultingList.append(hash(document[i:i+SHINGLE_LENGTH]))\n",
    "    i += 1\n",
    "  return resultingList\n",
    "\n",
    "def minHash(listOfShingles, hashFunctions):\n",
    "  result = []\n",
    "  for ind in range(len(hashFunctions)):\n",
    "    minVal = float(\"inf\")\n",
    "    for shingle in listOfShingles:\n",
    "      hashResult = hashFunctions[ind](shingle)\n",
    "      if hashResult < minVal:\n",
    "        minVal = hashResult\n",
    "    result.append(minVal)\n",
    "  return result\n",
    "\n",
    "def getHashFunctions(number):\n",
    "  result = []\n",
    "  rand = random.Random()\n",
    "  for i in range(number):\n",
    "    a = rand.randint(0, PRIME-1)\n",
    "    b = rand.randint(0, PRIME-1)\n",
    "    result.append(lambda x, a=a, b=b: ((a * x + b) % PRIME))\n",
    "  return result\n",
    "\n",
    "def signatureToHashedBandsOfRows(signature, numberOfBands, numberOfRowsInBand):\n",
    "  if len(signature) != numberOfBands * numberOfRowsInBand:\n",
    "    raise Exception(\"Wrong arguments number of bands times number of rows should equal length of signature\")\n",
    "  i = 0\n",
    "  bands = []\n",
    "  while i + numberOfRowsInBand <= len(signature):\n",
    "    bands.append(signature[i:i+numberOfRowsInBand])\n",
    "    i += numberOfRowsInBand\n",
    "  return [hash(tuple(band)) for band in bands]\n",
    "\n",
    "def findSimilarity(ints, ints1):\n",
    "  print(ints, ints1)\n",
    "  intersection = set(ints).intersection(set(ints1))\n",
    "  union = set(ints).union(set(ints1))\n",
    "  return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+--------------------+\n",
      "|                text| id|            shingles|             minHash|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "|you could create ...|  0|[-451277881344813...|[27707836, 331706...|\n",
      "|you could do a da...|  1|[-451277881344813...|[27707836, 331706...|\n",
      "|the document simi...|  2|[6618406970764183...|[141778275, 33170...|\n",
      "|  the lazy black cat|  3|[6618406970764183...|[15180064, 164757...|\n",
      "+--------------------+---+--------------------+--------------------+\n",
      "\n",
      "+---+---+-----------+\n",
      "| id| id| similarity|\n",
      "+---+---+-----------+\n",
      "|  0|  1| 0.73913044|\n",
      "|  0|  2| 0.05263158|\n",
      "|  0|  3|0.025641026|\n",
      "|  1|  2| 0.08108108|\n",
      "|  1|  3|0.025641026|\n",
      "|  2|  3|        0.0|\n",
      "+---+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import LongType, FloatType\n",
    "\n",
    "# sharedHashFunctions = sc.broadcast(getHashFunctions(NUMBER_OF_HASHFUNCTIONS))\n",
    "sharedHashFunctions = getHashFunctions(NUMBER_OF_HASHFUNCTIONS)\n",
    "\n",
    "text_df = spark.read.text(\"../dataset/text.txt\").withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "# 自定义 UDF，为每行分配一个连续的整数 ID\n",
    "counter = iter(range(text_df.count()))\n",
    "row_id_udf = udf(lambda _: next(counter), LongType())\n",
    "\n",
    "# 为 DataFrame 增加行号\n",
    "text_df = text_df.withColumn(\"id\", row_id_udf(\"text\"))\n",
    "\n",
    "# 处理文本\n",
    "preprocessDocument_udf = udf(preprocessDocument, StringType())\n",
    "text_df = text_df.withColumn(\"text\", preprocessDocument_udf(\"text\"))\n",
    "\n",
    "# 分词\n",
    "shingle_udf = udf(shingle, ArrayType(LongType()))\n",
    "text_df = text_df.withColumn(\"shingles\", shingle_udf(\"text\"))\n",
    "\n",
    "# MinHash\n",
    "minHash_udf = udf(lambda x: minHash(x, sharedHashFunctions), ArrayType(LongType()))\n",
    "text_df = text_df.withColumn(\"minHash\", minHash_udf(\"shingles\"))\n",
    "text_df.show()\n",
    "\n",
    "# 相似度 udf\n",
    "jaccard_similarity = udf(lambda x, y: len(\n",
    "            set(x).intersection(set(y))) / len(set(x).union(set(y))), FloatType())\n",
    "\n",
    "# 相似度\n",
    "similarities = text_df.alias(\"left\").crossJoin(text_df.alias(\"right\")) \\\n",
    "    .filter(col(\"right.id\") > col(\"left.id\"))  \\\n",
    "    .withColumn(\"similarity\", jaccard_similarity(col(\"left.minHash\"), col(\"right.minHash\")))  \\\n",
    "    # .filter(col(\"similarity\") > SIMILARITY_THRESHOLD)\n",
    "\n",
    "similarities.select(\"left.id\", \"right.id\", \"similarity\").show()\n",
    "\n",
    "# bands = minHashed.map(lambda x: (x[0], signatureToHashedBandsOfRows(x[1], NUMBER_OF_BANDS, NUMBER_OF_ROWS)))\n",
    "# bands.cache()\n",
    "# similarities.toDF().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe 封装成 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SHINGLE_LENGTH = 3\n",
    "PRIME = 2147483647\n",
    "SIMILARITY_THRESHOLD = 0.001\n",
    "BAND_SIMILARITY_THRESHOLD = 0.001\n",
    "NUMBER_OF_BANDS = 10\n",
    "NUMBER_OF_ROWS = 5\n",
    "NUMBER_OF_HASHFUNCTIONS = 20\n",
    "\n",
    "class minLSH:\n",
    "    def __init__(self, numHashTables=5, shingleSize=5, inputCol=\"features\", outputCol=\"hashes\"):\n",
    "        self.numHashTables = numHashTables\n",
    "        self.shingleSize = shingleSize\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.hashFunctions = self.getHashFunctions(self.numHashTables)\n",
    "\n",
    "    def getHashFunctions(self, number):\n",
    "        result = []\n",
    "        rand = random.Random()\n",
    "        for _ in range(number):\n",
    "            a = rand.randint(0, PRIME-1)\n",
    "            b = rand.randint(0, PRIME-1)\n",
    "            result.append(lambda x, a=a, b=b: ((a * x + b) % PRIME))\n",
    "        return result\n",
    "    \n",
    "    def preprocessDocument(self, document):\n",
    "        return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \") if isinstance(document, str) else document\n",
    "\n",
    "    def shingle(self, document):\n",
    "        resultingList = []\n",
    "        i = 0\n",
    "        while i + self.shingleSize < len(document):\n",
    "            resultingList.append(hash(document[i:i+self.shingleSize]))\n",
    "            i += 1\n",
    "        return resultingList\n",
    "\n",
    "    def minHash(self, listOfShingles):\n",
    "        result = []\n",
    "        for ind in range(len(self.hashFunctions)):\n",
    "            minVal = float(\"inf\")\n",
    "            for shingle in listOfShingles:\n",
    "                hashResult = self.hashFunctions[ind](shingle)\n",
    "                if hashResult < minVal:\n",
    "                    minVal = hashResult\n",
    "            result.append(minVal)\n",
    "        return result\n",
    "    \n",
    "    def signatureToHashedBandsOfRows(self, signature, numberOfBands, numberOfRowsInBand):\n",
    "        if len(signature) != numberOfBands * numberOfRowsInBand:\n",
    "            raise Exception(\"Wrong arguments number of bands times number of rows should equal length of signature\")\n",
    "        i = 0\n",
    "        bands = []\n",
    "        while i + numberOfRowsInBand <= len(signature):\n",
    "            bands.append(signature[i:i+numberOfRowsInBand])\n",
    "            i += numberOfRowsInBand\n",
    "        return [hash(tuple(band)) for band in bands]\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        preprocessDocument_udf = udf(self.preprocessDocument, StringType())\n",
    "        df = dataframe.withColumn(\"document\", preprocessDocument_udf(self.inputCol))\n",
    "        # 分词\n",
    "        shingle_udf = udf(self.shingle, ArrayType(LongType()))\n",
    "        df = df.withColumn(\"shingles\", shingle_udf(\"document\")).drop(\"document\")\n",
    "        # MinHash\n",
    "        minHash_udf = udf(lambda x: self.minHash(x), ArrayType(LongType()))\n",
    "        df = df.withColumn(\"minHash\", minHash_udf(\"shingles\")).drop(\"shingles\")\n",
    "        return df\n",
    "\n",
    "    def approxSimilarityJoin(self, dataframeA, dataframeB, threshold):\n",
    "        # 相似度 udf\n",
    "        jaccard_similarity = udf(lambda x, y: len(\n",
    "                    set(x).intersection(set(y))) / len(set(x).union(set(y))), FloatType())\n",
    "\n",
    "        # 相似度\n",
    "        similarities = dataframeA.alias(\"left\").crossJoin(dataframeB.alias(\"right\")) \\\n",
    "            .withColumn(\"similarity\", jaccard_similarity(col(\"left.minHash\"), col(\"right.minHash\")))  \\\n",
    "            .filter(col(\"similarity\") >= threshold)\n",
    "\n",
    "        return similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+\n",
      "|                text| id|             minHash|\n",
      "+--------------------+---+--------------------+\n",
      "|you could create ...|  0|[57937548, 580122...|\n",
      "|you could do a da...|  1|[57937548, 580122...|\n",
      "|the document simi...|  2|[30349375, 778723...|\n",
      "|  The lazy black cat|  3|[23535873, 778723...|\n",
      "+--------------------+---+--------------------+\n",
      "\n",
      "+---+---+----------+\n",
      "| id| id|similarity|\n",
      "+---+---+----------+\n",
      "|  1|  0| 0.8181818|\n",
      "|  2|  0|       0.0|\n",
      "|  2|  1|       0.0|\n",
      "|  3|  0|       0.0|\n",
      "|  3|  1|       0.0|\n",
      "|  3|  2|0.05263158|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = spark.read.text(\"../dataset/text.txt\").withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "# 自定义 UDF，为每行分配一个连续的整数 ID\n",
    "counter = iter(range(text_df.count()))\n",
    "row_id_udf = udf(lambda _: next(counter), LongType())\n",
    "\n",
    "# 为 DataFrame 增加行号\n",
    "text_df = text_df.withColumn(\"id\", row_id_udf(\"text\"))\n",
    "model = minLSH(numHashTables=10, shingleSize=3, inputCol=\"text\", outputCol=\"hashes\")\n",
    "model = model.fit(text_df)\n",
    "text_df = model.transform(text_df)\n",
    "text_df.show()\n",
    "result = model.approxSimilarityJoin(text_df, text_df, 0)\n",
    "result.where(\"left.id>right.id\").select(\"left.id\", \"right.id\", \"similarity\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
