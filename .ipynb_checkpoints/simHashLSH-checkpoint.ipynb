{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc92581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# initialize spark context\n",
    "sc = SparkContext(\"local\", \"simhash_lsh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a2135260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "+------------+------------+--------+\n",
      "|document_id1|document_id2|distance|\n",
      "+------------+------------+--------+\n",
      "|document1   |document2   |12      |\n",
      "|document1   |document3   |23      |\n",
      "|document1   |document4   |29      |\n",
      "|document2   |document3   |25      |\n",
      "|document2   |document4   |35      |\n",
      "|document3   |document4   |28      |\n",
      "+------------+------------+--------+\n",
      "\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|document_id|            document|             simhash|    bucket_0|    bucket_1|    bucket_2|    bucket_3|    bucket_4|    bucket_5|    bucket_6|    bucket_7|    bucket_8|    bucket_9|   bucket_10|   bucket_11|   bucket_12|   bucket_13|   bucket_14|   bucket_15|\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|  document1|you could create ...|[0, 1, 0, 0, 1, 0...|[0, 1, 0, 0]|[1, 0, 0, 1]|[1, 0, 0, 1]|[1, 0, 0, 1]|[0, 1, 0, 1]|[1, 1, 1, 0]|[0, 0, 1, 1]|[1, 0, 1, 0]|[1, 1, 1, 1]|[0, 1, 1, 1]|[0, 1, 0, 0]|[1, 0, 0, 0]|[0, 0, 1, 0]|[1, 0, 1, 0]|[1, 1, 0, 0]|[0, 1, 0, 1]|\n",
      "|  document2|you could do a da...|[0, 1, 0, 0, 1, 0...|[0, 1, 0, 0]|[1, 0, 0, 1]|[1, 0, 0, 0]|[0, 0, 1, 0]|[1, 1, 0, 1]|[1, 1, 1, 1]|[0, 0, 1, 1]|[1, 0, 1, 1]|[1, 1, 1, 1]|[0, 1, 1, 0]|[0, 1, 0, 1]|[1, 0, 1, 0]|[0, 0, 0, 0]|[1, 0, 1, 1]|[1, 1, 0, 0]|[0, 1, 0, 1]|\n",
      "|  document3|the document simi...|[0, 1, 0, 1, 1, 0...|[0, 1, 0, 1]|[1, 0, 0, 1]|[1, 1, 0, 1]|[1, 1, 0, 1]|[0, 1, 1, 0]|[1, 1, 0, 0]|[0, 0, 0, 1]|[0, 0, 1, 0]|[0, 1, 0, 0]|[0, 1, 1, 0]|[0, 1, 0, 1]|[1, 1, 1, 0]|[0, 0, 0, 0]|[0, 1, 0, 1]|[0, 0, 0, 1]|[0, 1, 0, 1]|\n",
      "|  document4|  The lazy black cat|[0, 0, 0, 1, 0, 1...|[0, 0, 0, 1]|[0, 1, 0, 0]|[0, 0, 1, 0]|[1, 1, 0, 0]|[0, 1, 1, 0]|[0, 1, 1, 0]|[0, 0, 0, 1]|[0, 0, 1, 0]|[0, 0, 1, 0]|[0, 1, 0, 1]|[1, 0, 1, 0]|[1, 0, 0, 1]|[0, 0, 0, 0]|[1, 1, 1, 0]|[1, 0, 0, 1]|[0, 1, 1, 0]|\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 964:=======================================>             (74 + 10) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------+--------------------+\n",
      "|document_id|document_id|distance|matching_buckets_sum|\n",
      "+-----------+-----------+--------+--------------------+\n",
      "|  document1|  document2|      12|                   6|\n",
      "|  document1|  document3|      23|                   2|\n",
      "|  document1|  document4|      29|                   0|\n",
      "|  document2|  document3|      25|                   5|\n",
      "|  document2|  document4|      35|                   1|\n",
      "|  document3|  document4|      28|                   4|\n",
      "+-----------+-----------+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 964:=====================================================>(99 + 1) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col, split, concat_ws, substring, expr, when\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "# data = [\n",
    "#     (\"document1\", \"The quick brown fox\"),\n",
    "#     (\"document2\", \"The lazy black dog\"),\n",
    "#     (\"document3\", \"The quick brown cat\"),\n",
    "#     (\"document4\", \"The lazy black cat\"),\n",
    "# ]\n",
    "data = [\n",
    "    (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "    (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "    (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"document\"]))\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 计算函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 计算文档之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.document_id\") < col(\"b.document_id\")) \\\n",
    "    .select(col(\"a.document_id\").alias(\"document_id1\"),\n",
    "            col(\"b.document_id\").alias(\"document_id2\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "threshold = 64\n",
    "# similar_documents = similar_documents.filter(col(\"distance\") <= threshold)\n",
    "\n",
    "# 打印相似文档\n",
    "similar_documents.show(truncate=False)\n",
    "\n",
    "# 将 SimHash 值映射到桶(bucket)\n",
    "num_buckets = 16 # 桶(bucket)的数量\n",
    "\n",
    "# 使用 split 函数将数组列拆分为多个列，并计算每列应包含的元素数量\n",
    "array_len = len(df.first()['simhash'])\n",
    "elements_per_bucket = int(array_len / num_buckets)\n",
    "\n",
    "# 循环创建新的数组列，并使用 split 函数拆分源数组列中的元素\n",
    "for i in range(num_buckets):\n",
    "    # 计算拆分的起始和结束位置\n",
    "    start = i * elements_per_bucket\n",
    "    end = start + elements_per_bucket\n",
    "    # 使用 split 函数拆分数组列，并为新列命名\n",
    "    new_col_name = f\"bucket_{i}\"\n",
    "    df = df.withColumn(new_col_name, expr(f\"slice(simhash, {start + 1}, {end - start})\"))\n",
    "# df.show()\n",
    "\n",
    "# 要比较的buckets list\n",
    "columns_to_compare = [f\"bucket_{i}\" for i in range(num_buckets)]\n",
    "\n",
    "# 计算每两个documents之间有多少buckets相同\n",
    "matching_columns_expr = [expr(f\"CASE WHEN df1.{col_name} = df2.{col_name} THEN 1 ELSE 0 END as matching_buckets_{col_name}\") for col_name in columns_to_compare]\n",
    "\n",
    "# 使用 select 函数选择需要的列，并显示结果\n",
    "# df_result = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.document_id\") < col(\"df2.document_id\"))\\\n",
    "#     .select(\"df1.document_id\", \"df2.document_id\", *matching_columns_expr)\n",
    "# df_result.show()\n",
    "\n",
    "# 累加求和操作\n",
    "matching_columns_sum_expr = \"+\".join([f\"matching_buckets_{col_name}\" for col_name in columns_to_compare])\n",
    "# 使用 join 函数连接两个 DataFrame，并使用 groupBy 和 sum 函数进行累加求和操作\n",
    "df = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.document_id\") < col(\"df2.document_id\"))\\\n",
    "    .select('*', *matching_columns_expr)\\\n",
    "    .groupBy(\"df1.document_id\", \"df2.document_id\", hamming_distance_udf(col(\"df1.simhash\"), col(\"df2.simhash\")).alias(\"distance\"))\\\n",
    "    .agg(expr(f\"sum({'+'.join([f'matching_buckets_{col_name}' for col_name in columns_to_compare])}) as matching_buckets_sum\"))\n",
    "\n",
    "# 显示结果\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a32c8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+--------+\n",
      "|document_id1|document_id2|similarity|distance|\n",
      "+------------+------------+----------+--------+\n",
      "|document1   |document2   |0.8125    |12      |\n",
      "|document1   |document3   |0.640625  |23      |\n",
      "|document1   |document4   |0.546875  |29      |\n",
      "|document2   |document3   |0.609375  |25      |\n",
      "|document2   |document4   |0.453125  |35      |\n",
      "|document3   |document4   |0.5625    |28      |\n",
      "+------------+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "# data = [\n",
    "#     (\"document1\", \"The quick brown fox\"),\n",
    "#     (\"document2\", \"The lazy black dog\"),\n",
    "#     (\"document3\", \"The quick brown cat\"),\n",
    "#     (\"document4\", \"The lazy black cat\"),\n",
    "# ]\n",
    "data = [\n",
    "    (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "    (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "    (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"document\"]))\n",
    "\n",
    "# # 创建 Tokenizer 特征提取器\n",
    "# tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\n",
    "\n",
    "# # 对文档进行分词\n",
    "# df = tokenizer.transform(df)\n",
    "\n",
    "# # 将 SimHash 值映射到桶(bucket)\n",
    "# num_buckets = 4 # 桶(bucket)的数量\n",
    "# hashingTF = HashingTF(inputCol=\"simhash\", outputCol=\"hashed_features\", numFeatures=num_buckets)\n",
    "# df = hashingTF.transform(df)\n",
    "# df.show()\n",
    "\n",
    "# 定义 Jaccard 相似度计算函数\n",
    "def jaccard_similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 将 Jaccard 相似度计算函数注册为 UDF\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "\n",
    "# 计算文档之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.document_id\") < col(\"b.document_id\")) \\\n",
    "    .select(col(\"a.document_id\").alias(\"document_id1\"),\n",
    "            col(\"b.document_id\").alias(\"document_id2\"),\n",
    "            jaccard_similarity_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "threshold = 20\n",
    "# similar_documents = similar_documents.filter(col(\"distance\") < threshold)\n",
    "\n",
    "# 打印相似文档\n",
    "similar_documents.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b95f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|              image1|              image2|similarity|distance|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|[15, 19, 24, 18, ...|[107, 103, 103, 1...|  0.953125|       3|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 加载示例图像\n",
    "image1 = Image.open(\"data/test1.png\")\n",
    "image2 = Image.open(\"data/test2.png\")\n",
    "\n",
    "# 将图像缩放为 32x32 并转换为灰度图像\n",
    "image1 = image1.resize((32, 32)).convert(\"L\")\n",
    "image2 = image2.resize((32, 32)).convert(\"L\")\n",
    "\n",
    "# 将图像数据转换为 NumPy 数组\n",
    "image1_array = np.array(image1)\n",
    "image2_array = np.array(image2)\n",
    "\n",
    "# 将图像数据展平为一维数组\n",
    "image1_vector = image1_array.flatten().tolist()\n",
    "image2_vector = image2_array.flatten().tolist()\n",
    "\n",
    "print(len(image1_vector))\n",
    "\n",
    "# 将图像数据转换为 DataFrame\n",
    "df = spark.createDataFrame([(image1_vector,), (image2_vector,)], [\"image\"])\n",
    "\n",
    "# 注册 DataFrame 为临时表\n",
    "df.createOrReplaceTempView(\"image_data\")\n",
    "\n",
    "# 将图像数据转换为 SimHash 值\n",
    "def simhash(image_vector):\n",
    "    # 将像素值转换为 SimHash 值\n",
    "    hashes = []\n",
    "    for pixel in image_vector:\n",
    "        pixel_hash = hash(pixel)\n",
    "        # 使用 64 位 SimHash 值，将像素值的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(pixel_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"image\"]))\n",
    "\n",
    "# 定义 Jaccard 相似度计算函数\n",
    "def jaccard_similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 计算 SimHash 值之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.image\") < col(\"b.image\")) \\\n",
    "    .select(col(\"a.image\").alias(\"image1\"),\n",
    "            col(\"b.image\").alias(\"image2\"),\n",
    "            jaccard_similarity_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "df = df.alias(\"df1\").crossJoin(df.alias(\"df2\"))\n",
    "df = df.filter(col(\"df1.image\") < col(\"df2.image\"))  # 排除与自身比较的情况\n",
    "df = df.withColumn(\"distance\", hamming_distance_udf(col(\"df1.simhash\"), col(\"df2.simhash\")))\n",
    "\n",
    "similar_documents.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
