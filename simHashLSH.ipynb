{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea982b1",
   "metadata": {},
   "source": [
    "### 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc92581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "data = [\n",
    "    (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "    (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "    (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "dfA = spark.read.csv(\"../dataset/SICK_train.csv\", sep=\"\t\", header=True).select(\"pair_ID\", \"sentence_A\")\n",
    "dfB = spark.read.csv(\"../dataset/SICK_train.csv\", sep=\"\t\", header=True).select(\"pair_ID\", \"sentence_B\")\n",
    "\n",
    "def preprocessDocument(document):\n",
    "    return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025f3dd",
   "metadata": {},
   "source": [
    "### 不分桶的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32c8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+----------+--------+\n",
      "|pair_ID|sentence_A                                                                                |sentence_B                                                                         |similarity|distance|\n",
      "+-------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+----------+--------+\n",
      "|1      |A group of kids is playing in a yard and an old man is standing in the background         |A group of boys in a yard is playing and a man is standing in the background       |0.890625  |7       |\n",
      "|2      |A group of children is playing in the house and there is no man standing in the background|A group of kids is playing in a yard and an old man is standing in the background  |0.828125  |11      |\n",
      "|3      |The young boys are playing outdoors and the man is smiling nearby                         |The kids are playing outdoors near a man with a smile                              |0.578125  |27      |\n",
      "|5      |The kids are playing outdoors near a man with a smile                                     |A group of kids is playing in a yard and an old man is standing in the background  |0.6875    |20      |\n",
      "|9      |The young boys are playing outdoors and the man is smiling nearby                         |A group of kids is playing in a yard and an old man is standing in the background  |0.734375  |17      |\n",
      "|12     |Two dogs are fighting                                                                     |Two dogs are wrestling and hugging                                                 |0.75      |16      |\n",
      "|14     |A brown dog is attacking another animal in front of the man in pants                      |Two dogs are fighting                                                              |0.625     |24      |\n",
      "|18     |A brown dog is attacking another animal in front of the man in pants                      |Two dogs are wrestling and hugging                                                 |0.5       |32      |\n",
      "|25     |Nobody is riding the bicycle on one wheel                                                 |A person in a black jacket is doing tricks on a motorbike                          |0.578125  |27      |\n",
      "|26     |A person is riding the bicycle on one wheel                                               |A man in a black jacket is doing tricks on a motorbike                             |0.65625   |22      |\n",
      "|28     |A person on a black motorbike is doing tricks with a jacket                               |A person is riding the bicycle on one wheel                                        |0.703125  |19      |\n",
      "|30     |A man with a jersey is dunking the ball at a basketball game                              |The ball is being dunked by a man with a jersey at a basketball game               |0.890625  |7       |\n",
      "|35     |A man with a jersey is dunking the ball at a basketball game                              |A man who is playing dunks the basketball into the net and a crowd is in background|0.796875  |13      |\n",
      "|40     |The player is dunking the basketball into the net and a crowd is in background            |A man with a jersey is dunking the ball at a basketball game                       |0.75      |16      |\n",
      "|42     |Two people are kickboxing and spectators are not watching                                 |Two people are kickboxing and spectators are watching                              |0.921875  |5       |\n",
      "|44     |Two young women are sparring in a kickboxing fight                                        |Two women are sparring in a kickboxing match                                       |0.8125    |12      |\n",
      "|45     |Two young women are not sparring in a kickboxing fight                                    |Two women are sparring in a kickboxing match                                       |0.796875  |13      |\n",
      "|47     |Two people are kickboxing and spectators are watching                                     |Two young women are not sparring in a kickboxing fight                             |0.71875   |18      |\n",
      "|49     |Two women are sparring in a kickboxing match                                              |Two people are kickboxing and spectators are not watching                          |0.75      |16      |\n",
      "|55     |Three boys are jumping in the leaves                                                      |Three kids are jumping in the leaves                                               |0.828125  |11      |\n",
      "+-------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    document = preprocessDocument(document)\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "dfA = dfA.withColumn(\"simhash_A\", simhash_udf(dfA[\"sentence_A\"]))\n",
    "dfB = dfB.withColumn(\"simhash_B\", simhash_udf(dfB[\"sentence_B\"]))\n",
    "\n",
    "# 定义相似度计算函数\n",
    "def similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 将相似度计算函数注册为 UDF\n",
    "similarity_udf = udf(similarity, FloatType())\n",
    "\n",
    "# 计算文档之间的相似度\n",
    "similar_documents = dfA.join(dfB, 'pair_ID') \\\n",
    "    .select('pair_ID', 'sentence_A', 'sentence_B', \n",
    "            similarity_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"distance\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "# threshold = 20\n",
    "# similar_documents = similar_documents.filter(col(\"distance\") < threshold)\n",
    "\n",
    "# 打印相似文档\n",
    "similar_documents.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c582227",
   "metadata": {},
   "source": [
    "### 分桶的版本\n",
    "一共n个桶，假设规定海明距离小于k的文本为相似文本，则两个数据相似的条件为至少n-k个桶中的内容相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2135260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "+-------+--------+--------------------+\n",
      "|pair_ID|distance|matching_buckets_sum|\n",
      "+-------+--------+--------------------+\n",
      "|   2957|       6|                  11|\n",
      "|   7222|       5|                  11|\n",
      "|   7556|       3|                  13|\n",
      "|   1501|       7|                  11|\n",
      "|   4163|       6|                  12|\n",
      "|   6104|       5|                  11|\n",
      "|   7839|       5|                  11|\n",
      "|   9006|       4|                  13|\n",
      "|   2675|       2|                  14|\n",
      "|   2175|       2|                  14|\n",
      "|   1048|       5|                  11|\n",
      "|   1946|       3|                  13|\n",
      "|   3627|       6|                  11|\n",
      "|   9394|       6|                  11|\n",
      "|    857|       6|                  11|\n",
      "|   2828|       3|                  13|\n",
      "|   5248|       6|                  11|\n",
      "|   7287|       7|                  11|\n",
      "|   9497|       4|                  13|\n",
      "|    772|       9|                  11|\n",
      "+-------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col, split, concat_ws, substring, expr, when\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "data = [\n",
    "    (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "    (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "    (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "dfA = dfA.withColumn(\"simhash_A\", simhash_udf(dfA[\"sentence_A\"]))\n",
    "dfB = dfB.withColumn(\"simhash_B\", simhash_udf(dfB[\"sentence_B\"]))\n",
    "\n",
    "# 设置距离的阈值\n",
    "thres_distance = 5\n",
    "\n",
    "# 将 SimHash 值映射到桶(bucket)\n",
    "num_buckets = 16 # 桶(bucket)的数量\n",
    "\n",
    "# 使用 split 函数将数组列拆分为多个列，并计算每列应包含的元素数量\n",
    "array_len = len(dfA.first()['simhash_A'])\n",
    "elements_per_bucket = int(array_len / num_buckets)\n",
    "\n",
    "# 循环创建新的数组列，并使用 split 函数拆分源数组列中的元素\n",
    "for i in range(num_buckets):\n",
    "    # 计算拆分的起始和结束位置\n",
    "    start = i * elements_per_bucket\n",
    "    end = start + elements_per_bucket\n",
    "    # 使用 split 函数拆分数组列，并为新列命名\n",
    "    new_col_name = f\"bucket_{i}\"\n",
    "    dfA = dfA.withColumn(f\"{new_col_name}_A\", expr(f\"slice(simhash_A, {start + 1}, {end - start})\"))\n",
    "    dfB = dfB.withColumn(f\"{new_col_name}_B\", expr(f\"slice(simhash_B, {start + 1}, {end - start})\"))\n",
    "# df.show()\n",
    "\n",
    "# 要比较的buckets list\n",
    "columns_to_compare = [f\"bucket_{i}\" for i in range(num_buckets)]\n",
    "\n",
    "# 计算每两个documents之间有多少buckets相同\n",
    "matching_columns_expr = [expr(f\"CASE WHEN {col_name}_A = {col_name}_B THEN 1 ELSE 0 END as matching_buckets_{col_name}\") for col_name in columns_to_compare]\n",
    "\n",
    "# 使用 select 函数选择需要的列，并显示结果\n",
    "# df_result = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.document_id\") < col(\"df2.document_id\"))\\\n",
    "#     .select(\"df1.document_id\", \"df2.document_id\", *matching_columns_expr)\n",
    "# df_result.show()\n",
    "\n",
    "# 累加求和操作\n",
    "matching_columns_sum_expr = \"+\".join([f\"matching_buckets_{col_name}\" for col_name in columns_to_compare])\n",
    "# 使用 join 函数连接两个 DataFrame，并使用 groupBy 和 sum 函数进行累加求和操作\n",
    "df = dfA.join(dfB, 'pair_ID')\\\n",
    "    .select('*', *matching_columns_expr)\\\n",
    "    .groupBy(\"pair_ID\", hamming_distance_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"distance\"))\\\n",
    "    .agg(expr(f\"sum({'+'.join([f'matching_buckets_{col_name}' for col_name in columns_to_compare])}) as matching_buckets_sum\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "threshold = num_buckets - thres_distance\n",
    "df = df.filter(col(\"matching_buckets_sum\") >= threshold)\n",
    "\n",
    "# 显示结果\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e088aa5",
   "metadata": {},
   "source": [
    "### 图像相似度对比（不用了）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b95f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|              image1|              image2|similarity|distance|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|[15, 19, 24, 18, ...|[107, 103, 103, 1...|  0.953125|       3|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 加载示例图像\n",
    "image1 = Image.open(\"data/test1.png\")\n",
    "image2 = Image.open(\"data/test2.png\")\n",
    "\n",
    "# 将图像缩放为 32x32 并转换为灰度图像\n",
    "image1 = image1.resize((32, 32)).convert(\"L\")\n",
    "image2 = image2.resize((32, 32)).convert(\"L\")\n",
    "\n",
    "# 将图像数据转换为 NumPy 数组\n",
    "image1_array = np.array(image1)\n",
    "image2_array = np.array(image2)\n",
    "\n",
    "# 将图像数据展平为一维数组\n",
    "image1_vector = image1_array.flatten().tolist()\n",
    "image2_vector = image2_array.flatten().tolist()\n",
    "\n",
    "print(len(image1_vector))\n",
    "\n",
    "# 将图像数据转换为 DataFrame\n",
    "df = spark.createDataFrame([(image1_vector,), (image2_vector,)], [\"image\"])\n",
    "\n",
    "# 注册 DataFrame 为临时表\n",
    "df.createOrReplaceTempView(\"image_data\")\n",
    "\n",
    "# 将图像数据转换为 SimHash 值\n",
    "def simhash(image_vector):\n",
    "    # 将像素值转换为 SimHash 值\n",
    "    hashes = []\n",
    "    for pixel in image_vector:\n",
    "        pixel_hash = hash(pixel)\n",
    "        # 使用 64 位 SimHash 值，将像素值的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(pixel_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"image\"]))\n",
    "\n",
    "# 定义 Jaccard 相似度计算函数\n",
    "def jaccard_similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 计算 SimHash 值之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.image\") < col(\"b.image\")) \\\n",
    "    .select(col(\"a.image\").alias(\"image1\"),\n",
    "            col(\"b.image\").alias(\"image2\"),\n",
    "            jaccard_similarity_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "df = df.alias(\"df1\").crossJoin(df.alias(\"df2\"))\n",
    "df = df.filter(col(\"df1.image\") < col(\"df2.image\"))  # 排除与自身比较的情况\n",
    "df = df.withColumn(\"distance\", hamming_distance_udf(col(\"df1.simhash\"), col(\"df2.simhash\")))\n",
    "\n",
    "similar_documents.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
