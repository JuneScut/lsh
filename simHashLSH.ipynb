{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc92581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# initialize spark context\n",
    "sc = SparkContext(\"local\", \"simhash_lsh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a32c8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+--------+\n",
      "|document_id1|document_id2|similarity|distance|\n",
      "+------------+------------+----------+--------+\n",
      "|document1   |document2   |0.8125    |12      |\n",
      "|document1   |document3   |0.640625  |23      |\n",
      "|document1   |document4   |0.546875  |29      |\n",
      "|document2   |document3   |0.609375  |25      |\n",
      "|document2   |document4   |0.453125  |35      |\n",
      "|document3   |document4   |0.5625    |28      |\n",
      "+------------+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "# data = [\n",
    "#     (\"document1\", \"The quick brown fox\"),\n",
    "#     (\"document2\", \"The lazy black dog\"),\n",
    "#     (\"document3\", \"The quick brown cat\"),\n",
    "#     (\"document4\", \"The lazy black cat\"),\n",
    "# ]\n",
    "data = [\n",
    "    (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "    (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "    (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"document\"]))\n",
    "\n",
    "# # 创建 Tokenizer 特征提取器\n",
    "# tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\n",
    "\n",
    "# # 对文档进行分词\n",
    "# df = tokenizer.transform(df)\n",
    "\n",
    "# # 将 SimHash 值映射到桶(bucket)\n",
    "# num_buckets = 4 # 桶(bucket)的数量\n",
    "# hashingTF = HashingTF(inputCol=\"simhash\", outputCol=\"hashed_features\", numFeatures=num_buckets)\n",
    "# df = hashingTF.transform(df)\n",
    "# df.show()\n",
    "\n",
    "# 定义 Jaccard 相似度计算函数\n",
    "def jaccard_similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 将 Jaccard 相似度计算函数注册为 UDF\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "\n",
    "# 计算文档之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.document_id\") < col(\"b.document_id\")) \\\n",
    "    .select(col(\"a.document_id\").alias(\"document_id1\"),\n",
    "            col(\"b.document_id\").alias(\"document_id2\"),\n",
    "            jaccard_similarity_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "threshold = 20\n",
    "# similar_documents = similar_documents.filter(col(\"distance\") < threshold)\n",
    "\n",
    "# 打印相似文档\n",
    "similar_documents.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0b99ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "+------------+------------+----------+--------+\n",
      "|document_id1|document_id2|similarity|distance|\n",
      "+------------+------------+----------+--------+\n",
      "|document1   |document2   |0.8125    |12      |\n",
      "|document1   |document3   |0.640625  |23      |\n",
      "|document1   |document4   |0.546875  |29      |\n",
      "|document2   |document3   |0.609375  |25      |\n",
      "|document2   |document4   |0.453125  |35      |\n",
      "|document3   |document4   |0.5625    |28      |\n",
      "+------------+------------+----------+--------+\n",
      "\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|document_id|            document|             simhash|    bucket_0|    bucket_1|    bucket_2|    bucket_3|    bucket_4|    bucket_5|    bucket_6|    bucket_7|    bucket_8|    bucket_9|   bucket_10|   bucket_11|   bucket_12|   bucket_13|   bucket_14|   bucket_15|\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|  document1|you could create ...|[0, 1, 0, 0, 1, 0...|[0, 1, 0, 0]|[1, 0, 0, 1]|[1, 0, 0, 1]|[1, 0, 0, 1]|[0, 1, 0, 1]|[1, 1, 1, 0]|[0, 0, 1, 1]|[1, 0, 1, 0]|[1, 1, 1, 1]|[0, 1, 1, 1]|[0, 1, 0, 0]|[1, 0, 0, 0]|[0, 0, 1, 0]|[1, 0, 1, 0]|[1, 1, 0, 0]|[0, 1, 0, 1]|\n",
      "|  document2|you could do a da...|[0, 1, 0, 0, 1, 0...|[0, 1, 0, 0]|[1, 0, 0, 1]|[1, 0, 0, 0]|[0, 0, 1, 0]|[1, 1, 0, 1]|[1, 1, 1, 1]|[0, 0, 1, 1]|[1, 0, 1, 1]|[1, 1, 1, 1]|[0, 1, 1, 0]|[0, 1, 0, 1]|[1, 0, 1, 0]|[0, 0, 0, 0]|[1, 0, 1, 1]|[1, 1, 0, 0]|[0, 1, 0, 1]|\n",
      "|  document3|the document simi...|[0, 1, 0, 1, 1, 0...|[0, 1, 0, 1]|[1, 0, 0, 1]|[1, 1, 0, 1]|[1, 1, 0, 1]|[0, 1, 1, 0]|[1, 1, 0, 0]|[0, 0, 0, 1]|[0, 0, 1, 0]|[0, 1, 0, 0]|[0, 1, 1, 0]|[0, 1, 0, 1]|[1, 1, 1, 0]|[0, 0, 0, 0]|[0, 1, 0, 1]|[0, 0, 0, 1]|[0, 1, 0, 1]|\n",
      "|  document4|  The lazy black cat|[0, 0, 0, 1, 0, 1...|[0, 0, 0, 1]|[0, 1, 0, 0]|[0, 0, 1, 0]|[1, 1, 0, 0]|[0, 1, 1, 0]|[0, 1, 1, 0]|[0, 0, 0, 1]|[0, 0, 1, 0]|[0, 0, 1, 0]|[0, 1, 0, 1]|[1, 0, 1, 0]|[1, 0, 0, 1]|[0, 0, 0, 0]|[1, 1, 1, 0]|[1, 0, 0, 1]|[0, 1, 1, 0]|\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "\n",
      "+-----------+-----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "|document_id|document_id|matching_buckets_bucket_0|matching_buckets_bucket_1|matching_buckets_bucket_2|matching_buckets_bucket_3|matching_buckets_bucket_4|matching_buckets_bucket_5|matching_buckets_bucket_6|matching_buckets_bucket_7|matching_buckets_bucket_8|matching_buckets_bucket_9|matching_buckets_bucket_10|matching_buckets_bucket_11|matching_buckets_bucket_12|matching_buckets_bucket_13|matching_buckets_bucket_14|matching_buckets_bucket_15|\n",
      "+-----------+-----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "|  document1|  document2|                        1|                        1|                        0|                        0|                        0|                        0|                        1|                        0|                        1|                        0|                         0|                         0|                         0|                         0|                         1|                         1|\n",
      "|  document1|  document3|                        0|                        1|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                         0|                         0|                         0|                         0|                         0|                         1|\n",
      "|  document1|  document4|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                         0|                         0|                         0|                         0|                         0|                         0|\n",
      "|  document2|  document3|                        0|                        1|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                        1|                         1|                         0|                         1|                         0|                         0|                         1|\n",
      "|  document2|  document4|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                        0|                         0|                         0|                         1|                         0|                         0|                         0|\n",
      "|  document3|  document4|                        0|                        0|                        0|                        0|                        1|                        0|                        1|                        1|                        0|                        0|                         0|                         0|                         1|                         0|                         0|                         0|\n",
      "+-----------+-----------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+-------------------------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+--------------------------+\n",
      "\n",
      "+-----------+-----------+--------------------+\n",
      "|document_id|document_id|matching_buckets_sum|\n",
      "+-----------+-----------+--------------------+\n",
      "|  document1|  document2|                   6|\n",
      "|  document1|  document3|                   2|\n",
      "|  document1|  document4|                   0|\n",
      "|  document2|  document3|                   5|\n",
      "|  document2|  document4|                   1|\n",
      "|  document3|  document4|                   4|\n",
      "+-----------+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 880:===================================================>  (96 + 4) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col, split, concat_ws, substring, expr, when\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "# data = [\n",
    "#     (\"document1\", \"The quick brown fox\"),\n",
    "#     (\"document2\", \"The lazy black dog\"),\n",
    "#     (\"document3\", \"The quick brown cat\"),\n",
    "#     (\"document4\", \"The lazy black cat\"),\n",
    "# ]\n",
    "data = [\n",
    "    (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "    (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "    (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"document\"]))\n",
    "\n",
    "# 定义 Jaccard 相似度计算函数 (hamming distance / length)\n",
    "def jaccard_similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 将 Jaccard 相似度计算函数注册为 UDF\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 计算函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 计算文档之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.document_id\") < col(\"b.document_id\")) \\\n",
    "    .select(col(\"a.document_id\").alias(\"document_id1\"),\n",
    "            col(\"b.document_id\").alias(\"document_id2\"),\n",
    "            jaccard_similarity_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "threshold = 0\n",
    "# similar_documents = similar_documents.filter(col(\"similarity\") > threshold)\n",
    "\n",
    "# 打印相似文档\n",
    "similar_documents.show(truncate=False)\n",
    "\n",
    "# 将 SimHash 值映射到桶(bucket)\n",
    "num_buckets = 16 # 桶(bucket)的数量\n",
    "\n",
    "# 使用 split 函数将数组列拆分为多个列，并计算每列应包含的元素数量\n",
    "array_len = len(df.first()['simhash'])\n",
    "elements_per_bucket = int(array_len / num_buckets)\n",
    "\n",
    "# 循环创建新的数组列，并使用 split 函数拆分源数组列中的元素\n",
    "for i in range(num_buckets):\n",
    "    # 计算拆分的起始和结束位置\n",
    "    start = i * elements_per_bucket\n",
    "    end = start + elements_per_bucket\n",
    "    # 使用 split 函数拆分数组列，并为新列命名\n",
    "    new_col_name = f\"bucket_{i}\"\n",
    "    df = df.withColumn(new_col_name, expr(f\"slice(simhash, {start + 1}, {end - start})\"))\n",
    "df.show()\n",
    "\n",
    "# 定义要比较的列名列表\n",
    "columns_to_compare = [f\"bucket_{i}\" for i in range(num_buckets)]\n",
    "\n",
    "# 计算每两行之间有多少列相同\n",
    "matching_columns_expr = [expr(f\"CASE WHEN df1.{col_name} = df2.{col_name} THEN 1 ELSE 0 END as matching_buckets_{col_name}\") for col_name in columns_to_compare]\n",
    "\n",
    "# 使用 select 函数选择需要的列，并显示结果\n",
    "df_result = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.document_id\") < col(\"df2.document_id\"))\\\n",
    "    .select(\"df1.document_id\", \"df2.document_id\", *matching_columns_expr)\n",
    "df_result.show()\n",
    "\n",
    "# 动态生成列名的累加求和操作\n",
    "matching_columns_sum_expr = \"+\".join([f\"matching_buckets_{col_name}\" for col_name in columns_to_compare])\n",
    "# 使用 join 函数连接两个 DataFrame，并使用 groupBy 和 sum 函数进行累加求和操作\n",
    "df_result = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.document_id\") < col(\"df2.document_id\"))\\\n",
    "    .select('*', *matching_columns_expr)\\\n",
    "    .groupBy(\"df1.document_id\", \"df2.document_id\")\\\n",
    "    .agg(expr(f\"sum({'+'.join([f'matching_buckets_{col_name}' for col_name in columns_to_compare])}) as matching_buckets_sum\"))\n",
    "\n",
    "# 显示结果\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a2135260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "+------------+------------+--------+\n",
      "|document_id1|document_id2|distance|\n",
      "+------------+------------+--------+\n",
      "|document1   |document2   |12      |\n",
      "|document1   |document3   |23      |\n",
      "|document1   |document4   |29      |\n",
      "|document2   |document3   |25      |\n",
      "|document2   |document4   |35      |\n",
      "|document3   |document4   |28      |\n",
      "+------------+------------+--------+\n",
      "\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|document_id|            document|             simhash|    bucket_0|    bucket_1|    bucket_2|    bucket_3|    bucket_4|    bucket_5|    bucket_6|    bucket_7|    bucket_8|    bucket_9|   bucket_10|   bucket_11|   bucket_12|   bucket_13|   bucket_14|   bucket_15|\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|  document1|you could create ...|[0, 1, 0, 0, 1, 0...|[0, 1, 0, 0]|[1, 0, 0, 1]|[1, 0, 0, 1]|[1, 0, 0, 1]|[0, 1, 0, 1]|[1, 1, 1, 0]|[0, 0, 1, 1]|[1, 0, 1, 0]|[1, 1, 1, 1]|[0, 1, 1, 1]|[0, 1, 0, 0]|[1, 0, 0, 0]|[0, 0, 1, 0]|[1, 0, 1, 0]|[1, 1, 0, 0]|[0, 1, 0, 1]|\n",
      "|  document2|you could do a da...|[0, 1, 0, 0, 1, 0...|[0, 1, 0, 0]|[1, 0, 0, 1]|[1, 0, 0, 0]|[0, 0, 1, 0]|[1, 1, 0, 1]|[1, 1, 1, 1]|[0, 0, 1, 1]|[1, 0, 1, 1]|[1, 1, 1, 1]|[0, 1, 1, 0]|[0, 1, 0, 1]|[1, 0, 1, 0]|[0, 0, 0, 0]|[1, 0, 1, 1]|[1, 1, 0, 0]|[0, 1, 0, 1]|\n",
      "|  document3|the document simi...|[0, 1, 0, 1, 1, 0...|[0, 1, 0, 1]|[1, 0, 0, 1]|[1, 1, 0, 1]|[1, 1, 0, 1]|[0, 1, 1, 0]|[1, 1, 0, 0]|[0, 0, 0, 1]|[0, 0, 1, 0]|[0, 1, 0, 0]|[0, 1, 1, 0]|[0, 1, 0, 1]|[1, 1, 1, 0]|[0, 0, 0, 0]|[0, 1, 0, 1]|[0, 0, 0, 1]|[0, 1, 0, 1]|\n",
      "|  document4|  The lazy black cat|[0, 0, 0, 1, 0, 1...|[0, 0, 0, 1]|[0, 1, 0, 0]|[0, 0, 1, 0]|[1, 1, 0, 0]|[0, 1, 1, 0]|[0, 1, 1, 0]|[0, 0, 0, 1]|[0, 0, 1, 0]|[0, 0, 1, 0]|[0, 1, 0, 1]|[1, 0, 1, 0]|[1, 0, 0, 1]|[0, 0, 0, 0]|[1, 1, 1, 0]|[1, 0, 0, 1]|[0, 1, 1, 0]|\n",
      "+-----------+--------------------+--------------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 964:=======================================>             (74 + 10) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------+--------------------+\n",
      "|document_id|document_id|distance|matching_buckets_sum|\n",
      "+-----------+-----------+--------+--------------------+\n",
      "|  document1|  document2|      12|                   6|\n",
      "|  document1|  document3|      23|                   2|\n",
      "|  document1|  document4|      29|                   0|\n",
      "|  document2|  document3|      25|                   5|\n",
      "|  document2|  document4|      35|                   1|\n",
      "|  document3|  document4|      28|                   4|\n",
      "+-----------+-----------+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 964:=====================================================>(99 + 1) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col, split, concat_ws, substring, expr, when\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "# data = [\n",
    "#     (\"document1\", \"The quick brown fox\"),\n",
    "#     (\"document2\", \"The lazy black dog\"),\n",
    "#     (\"document3\", \"The quick brown cat\"),\n",
    "#     (\"document4\", \"The lazy black cat\"),\n",
    "# ]\n",
    "data = [\n",
    "    (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "    (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "    (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"document\"]))\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 计算函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 计算文档之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.document_id\") < col(\"b.document_id\")) \\\n",
    "    .select(col(\"a.document_id\").alias(\"document_id1\"),\n",
    "            col(\"b.document_id\").alias(\"document_id2\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "threshold = 64\n",
    "# similar_documents = similar_documents.filter(col(\"distance\") <= threshold)\n",
    "\n",
    "# 打印相似文档\n",
    "similar_documents.show(truncate=False)\n",
    "\n",
    "# 将 SimHash 值映射到桶(bucket)\n",
    "num_buckets = 16 # 桶(bucket)的数量\n",
    "\n",
    "# 使用 split 函数将数组列拆分为多个列，并计算每列应包含的元素数量\n",
    "array_len = len(df.first()['simhash'])\n",
    "elements_per_bucket = int(array_len / num_buckets)\n",
    "\n",
    "# 循环创建新的数组列，并使用 split 函数拆分源数组列中的元素\n",
    "for i in range(num_buckets):\n",
    "    # 计算拆分的起始和结束位置\n",
    "    start = i * elements_per_bucket\n",
    "    end = start + elements_per_bucket\n",
    "    # 使用 split 函数拆分数组列，并为新列命名\n",
    "    new_col_name = f\"bucket_{i}\"\n",
    "    df = df.withColumn(new_col_name, expr(f\"slice(simhash, {start + 1}, {end - start})\"))\n",
    "# df.show()\n",
    "\n",
    "# 要比较的buckets list\n",
    "columns_to_compare = [f\"bucket_{i}\" for i in range(num_buckets)]\n",
    "\n",
    "# 计算每两个documents之间有多少buckets相同\n",
    "matching_columns_expr = [expr(f\"CASE WHEN df1.{col_name} = df2.{col_name} THEN 1 ELSE 0 END as matching_buckets_{col_name}\") for col_name in columns_to_compare]\n",
    "\n",
    "# 使用 select 函数选择需要的列，并显示结果\n",
    "# df_result = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.document_id\") < col(\"df2.document_id\"))\\\n",
    "#     .select(\"df1.document_id\", \"df2.document_id\", *matching_columns_expr)\n",
    "# df_result.show()\n",
    "\n",
    "# 累加求和操作\n",
    "matching_columns_sum_expr = \"+\".join([f\"matching_buckets_{col_name}\" for col_name in columns_to_compare])\n",
    "# 使用 join 函数连接两个 DataFrame，并使用 groupBy 和 sum 函数进行累加求和操作\n",
    "df = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.document_id\") < col(\"df2.document_id\"))\\\n",
    "    .select('*', *matching_columns_expr)\\\n",
    "    .groupBy(\"df1.document_id\", \"df2.document_id\", hamming_distance_udf(col(\"df1.simhash\"), col(\"df2.simhash\")).alias(\"distance\"))\\\n",
    "    .agg(expr(f\"sum({'+'.join([f'matching_buckets_{col_name}' for col_name in columns_to_compare])}) as matching_buckets_sum\"))\n",
    "\n",
    "# 显示结果\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b95f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|              image1|              image2|similarity|distance|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|[15, 19, 24, 18, ...|[107, 103, 103, 1...|  0.953125|       3|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 加载示例图像\n",
    "image1 = Image.open(\"data/test1.png\")\n",
    "image2 = Image.open(\"data/test2.png\")\n",
    "\n",
    "# 将图像缩放为 32x32 并转换为灰度图像\n",
    "image1 = image1.resize((32, 32)).convert(\"L\")\n",
    "image2 = image2.resize((32, 32)).convert(\"L\")\n",
    "\n",
    "# 将图像数据转换为 NumPy 数组\n",
    "image1_array = np.array(image1)\n",
    "image2_array = np.array(image2)\n",
    "\n",
    "# 将图像数据展平为一维数组\n",
    "image1_vector = image1_array.flatten().tolist()\n",
    "image2_vector = image2_array.flatten().tolist()\n",
    "\n",
    "print(len(image1_vector))\n",
    "\n",
    "# 将图像数据转换为 DataFrame\n",
    "df = spark.createDataFrame([(image1_vector,), (image2_vector,)], [\"image\"])\n",
    "\n",
    "# 注册 DataFrame 为临时表\n",
    "df.createOrReplaceTempView(\"image_data\")\n",
    "\n",
    "# 将图像数据转换为 SimHash 值\n",
    "def simhash(image_vector):\n",
    "    # 将像素值转换为 SimHash 值\n",
    "    hashes = []\n",
    "    for pixel in image_vector:\n",
    "        pixel_hash = hash(pixel)\n",
    "        # 使用 64 位 SimHash 值，将像素值的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(pixel_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"image\"]))\n",
    "\n",
    "# 定义 Jaccard 相似度计算函数\n",
    "def jaccard_similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 计算 SimHash 值之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.image\") < col(\"b.image\")) \\\n",
    "    .select(col(\"a.image\").alias(\"image1\"),\n",
    "            col(\"b.image\").alias(\"image2\"),\n",
    "            jaccard_similarity_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "df = df.alias(\"df1\").crossJoin(df.alias(\"df2\"))\n",
    "df = df.filter(col(\"df1.image\") < col(\"df2.image\"))  # 排除与自身比较的情况\n",
    "df = df.withColumn(\"distance\", hamming_distance_udf(col(\"df1.simhash\"), col(\"df2.simhash\")))\n",
    "\n",
    "similar_documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900e82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters for LSH\n",
    "num_bands = 10\n",
    "band_size = 8\n",
    "threshold = 0.8\n",
    "\n",
    "# define hash function for simhash\n",
    "def hash_function(x, r):\n",
    "    return (r * x + 1) % (2 ** 32)\n",
    "\n",
    "# define simhash function\n",
    "def simhash_feature_extraction(img_path, k=32):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((32, 32))  # resize to 32 x 32 pixels\n",
    "    pixels = np.array(img).flatten()  # flatten to 1D array\n",
    "    avg_pixel = pixels.mean()\n",
    "    diff = pixels > avg_pixel\n",
    "    simhash = np.zeros(k)\n",
    "    for i in range(len(diff)):\n",
    "        if diff[i]:\n",
    "            simhash += hash_function(i, np.arange(k))\n",
    "        else:\n",
    "            simhash -= hash_function(i, np.arange(k))\n",
    "    simhash[simhash >= 0] = 1\n",
    "    simhash[simhash < 0] = -1\n",
    "    return simhash\n",
    "\n",
    "# define function for generating candidates\n",
    "def generate_candidate_pairs(band_id, bands):\n",
    "    candidates = set()\n",
    "    for (img1, simhash1) in bands[band_id]:\n",
    "        for (img2, simhash2) in bands[band_id]:\n",
    "            print(simhash1.dot(simhash2))\n",
    "            if img1 != img2 and simhash1.dot(simhash2) >= threshold:\n",
    "                candidates.add((img1, img2))\n",
    "    return candidates\n",
    "\n",
    "def map_to_bands(x, band_size):\n",
    "    result = []\n",
    "    for i in range(0, len(x[1]), band_size):\n",
    "        band = tuple(x[1][i:i+band_size])\n",
    "        result.append(((i, band), x[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "204a2190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data/test1.png', array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])), ('data/test2.png', array([ 1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1.])), ('data/test3.png', array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))]\n",
      "[((0, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), 'data/test1.png'), ((8, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), 'data/test1.png'), ((16, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), 'data/test1.png'), ((24, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), 'data/test1.png'), ((0, (1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0)), 'data/test2.png'), ((8, (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0)), 'data/test2.png'), ((16, (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0)), 'data/test2.png'), ((24, (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0)), 'data/test2.png'), ((0, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), 'data/test3.png'), ((8, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), 'data/test3.png'), ((16, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), 'data/test3.png'), ((24, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), 'data/test3.png')]\n",
      "[((0, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), <pyspark.resultiterable.ResultIterable object at 0x7fdb18c5be80>), ((8, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), <pyspark.resultiterable.ResultIterable object at 0x7fdb18c5bee0>), ((16, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), <pyspark.resultiterable.ResultIterable object at 0x7fdb18c68100>), ((24, (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)), <pyspark.resultiterable.ResultIterable object at 0x7fdb18c5bf40>), ((0, (1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0)), <pyspark.resultiterable.ResultIterable object at 0x7fdb18c681c0>), ((8, (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0)), <pyspark.resultiterable.ResultIterable object at 0x7fdb18c683a0>), ((16, (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0)), <pyspark.resultiterable.ResultIterable object at 0x7fdb18c68580>), ((24, (-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0)), <pyspark.resultiterable.ResultIterable object at 0x7fdb18c5bfa0>)]\n"
     ]
    }
   ],
   "source": [
    "# load CIFAR-10 dataset and extract simhash features\n",
    "data_rdd = sc.parallelize(['data/test1.png', 'data/test2.png', 'data/test3.png'])\n",
    "features_rdd = data_rdd.map(lambda x: (x, simhash_feature_extraction(x)))\n",
    "bands_rdd = features_rdd.flatMap(lambda x: map_to_bands(x, band_size))\n",
    "# bands_rdd = features_rdd.flatMap(lambda x: [((i, tuple(x[1][i:i+band_size])), x[0]) for i in range(0, len(x[1]), band_size)])\n",
    "grouped_rdd = bands_rdd.groupByKey()\n",
    "# generate LSH candidates\n",
    "candidates_rdd = grouped_rdd.flatMap(lambda x: generate_candidate_pairs(x[0][0], list(x[1])))\n",
    "print(features_rdd.collect())\n",
    "print(bands_rdd.collect())\n",
    "print(grouped_rdd.collect())\n",
    "# print(candidates_rdd.collect())\n",
    "# print candidate pairs\n",
    "# candidates = candidates_rdd.collect()\n",
    "# for candidate in candidates:\n",
    "#     print(candidate)\n",
    "\n",
    "# stop spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "121e5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc05179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179cd6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "data = [\n",
    "    (\"document1\", \"The quick brown fox\"),\n",
    "    (\"document2\", \"The lazy black dog\"),\n",
    "    (\"document3\", \"The quick brown cat\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.extend(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = DenseVector(hashes)\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(FloatType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"document\"]))\n",
    "\n",
    "# 创建 Tokenizer 特征提取器\n",
    "tokenizer = Tokenizer(inputCol=\"document\", outputCol=\"words\")\n",
    "\n",
    "# 对文档进行分词\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# 将 SimHash 值映射到桶(bucket)\n",
    "num_buckets = 100 # 桶(bucket)的数量\n",
    "hashingTF = HashingTF(inputCol=\"simhash\", outputCol=\"hashed_features\", numFeatures=num_buckets)\n",
    "df = hashingTF.transform(df)\n",
    "\n",
    "# 定义 Jaccard 相似度计算函数\n",
    "def jaccard_similarity(set1, set2):\n",
    "    set1 = set(set1)\n",
    "    set2 = set(set2)\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1) + len(set2) - intersection_size\n",
    "    return intersection_size / union_size\n",
    "\n",
    "# 将 Jaccard 相似度计算函数注册为 UDF\n",
    "jaccard_similarity_udf = udf(jaccard_similarity, FloatType())\n",
    "\n",
    "# 计算 Jaccard 相似度\n",
    "df = df.alias(\"a\").crossJoin(df.alias(\"b\"))\n",
    "df = df.filter(\"a.document_id < b.document_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55e0556b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'd2.document_id' does not exist. Did you mean one of the following? [d1.document_id, d2.document, d1.document, d2.simhash, d1.simhash]; line 1 pos 0;\n'Project [document_id#270 AS document_id1#287, 'd2.document_id AS document_id2#288, hamming_distance_udf(simhash#275, simhash#281)#290 AS hamming_distance#289]\n+- Project [document_id#270, document#271, simhash#275, document#280, simhash#281]\n   +- Filter (document_id#270 < document_id#279)\n      +- Project [document_id#270, document#271, simhash#275, document#280, simhash#281, document_id#279]\n         +- Join Inner, (document_id#270 = document_id#279)\n            :- SubqueryAlias d1\n            :  +- Project [document_id#270, document#271, simhash(document#271)#274 AS simhash#275]\n            :     +- LogicalRDD [document_id#270, document#271], false\n            +- SubqueryAlias d2\n               +- Project [document_id#279, document#280, simhash(document#280)#274 AS simhash#281]\n                  +- LogicalRDD [document_id#279, document#280], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m spark\u001b[38;5;241m.\u001b[39mudf\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhamming_distance_udf\u001b[39m\u001b[38;5;124m\"\u001b[39m, hamming_distance_udf)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# 构建相似文档对的候选集\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m similar_docs \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md1.document_id < d2.document_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md1.document_id as document_id1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md2.document_id as document_id2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhamming_distance_udf(d1.simhash, d2.simhash) as hamming_distance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 设置阈值，筛选出相似文档对\u001b[39;00m\n\u001b[1;32m     62\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[0;32m~/BigData/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:2048\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2047\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/BigData/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/BigData/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'd2.document_id' does not exist. Did you mean one of the following? [d1.document_id, d2.document, d1.document, d2.simhash, d1.simhash]; line 1 pos 0;\n'Project [document_id#270 AS document_id1#287, 'd2.document_id AS document_id2#288, hamming_distance_udf(simhash#275, simhash#281)#290 AS hamming_distance#289]\n+- Project [document_id#270, document#271, simhash#275, document#280, simhash#281]\n   +- Filter (document_id#270 < document_id#279)\n      +- Project [document_id#270, document#271, simhash#275, document#280, simhash#281, document_id#279]\n         +- Join Inner, (document_id#270 = document_id#279)\n            :- SubqueryAlias d1\n            :  +- Project [document_id#270, document#271, simhash(document#271)#274 AS simhash#275]\n            :     +- LogicalRDD [document_id#270, document#271], false\n            +- SubqueryAlias d2\n               +- Project [document_id#279, document#280, simhash(document#280)#274 AS simhash#281]\n                  +- LogicalRDD [document_id#279, document#280], false\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from itertools import combinations\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSHExample\").getOrCreate()\n",
    "\n",
    "# 示例数据集\n",
    "data = [(\"doc1\", \"The quick brown fox jumps over the lazy dog\"),\n",
    "        (\"doc2\", \"The quick brown fox jumps over the quick dog\"),\n",
    "        (\"doc3\", \"The slow brown fox jumps over the lazy dog\"),\n",
    "        (\"doc4\", \"A brown fox jumps over a lazy dog\")]\n",
    "\n",
    "# 将数据集转换为 DataFrame\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    # 将文档转换为哈希特征向量\n",
    "    words = document.split(\" \")\n",
    "    words = [word for word in words if len(word) > 1]  # 过滤掉长度为 1 的单词\n",
    "    words = words[:10]  # 只取前 10 个单词进行 SimHash 计算\n",
    "    feature_vector = [0] * 64  # 初始化特征向量为全 0\n",
    "    for word in words:\n",
    "        # 计算单词的哈希值\n",
    "        word_hash = hash(word)\n",
    "        for i in range(64):\n",
    "            # 统计每一位的哈希值的二进制位\n",
    "            feature_vector[i] += 1 if (word_hash >> i) & 1 else -1\n",
    "    # 将特征向量转换为 SimHash 值\n",
    "    simhash_value = \"\".join([\"1\" if num >= 0 else \"0\" for num in feature_vector])\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, StringType())\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"document\"]))\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 注册 Hamming Distance UDF\n",
    "spark.udf.register(\"hamming_distance_udf\", hamming_distance_udf)\n",
    "\n",
    "# 构建相似文档对的候选集\n",
    "similar_docs = df.alias(\"d1\").join(df.alias(\"d2\"), \"document_id\") \\\n",
    "                .filter(\"d1.document_id < d2.document_id\") \\\n",
    "                .selectExpr(\"d1.document_id as document_id1\",\n",
    "                            \"d2.document_id as document_id2\",\n",
    "                            \"hamming_distance_udf(d1.simhash, d2.simhash) as hamming_distance\")\n",
    "\n",
    "# 设置阈值，筛选出相似文档对\n",
    "threshold = 3\n",
    "similar_docs = similar_docs.filter(f\"hamming_distance <= {threshold}\")\n",
    "\n",
    "# 显示相似的文档对\n",
    "similar_docs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "624ef59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/07 10:56:15 WARN SimpleFunctionRegistry: The function hamming_distance_udf replaced a previously registered function.\n",
      "+------------+------------+----------------+\n",
      "|document_id1|document_id2|hamming_distance|\n",
      "+------------+------------+----------------+\n",
      "+------------+------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:==================================================>     (68 + 7) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from itertools import combinations\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSHExample\").getOrCreate()\n",
    "\n",
    "# 示例数据集\n",
    "data = [(\"doc1\", \"The quick brown fox jumps over the lazy dog\"),\n",
    "        (\"doc2\", \"The quick brown fox jumps over the quick dog\"),\n",
    "        (\"doc3\", \"The slow brown fox jumps over the lazy dog\"),\n",
    "        (\"doc4\", \"A brown fox jumps over a lazy dog\")]\n",
    "\n",
    "# 将数据集转换为 DataFrame\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    # 将文档转换为哈希特征向量\n",
    "    words = document.split(\" \")\n",
    "    words = [word for word in words if len(word) > 1]  # 过滤掉长度为 1 的单词\n",
    "    words = words[:10]  # 只取前 10 个单词进行 SimHash 计算\n",
    "    feature_vector = [0] * 64  # 初始化特征向量为全 0\n",
    "    for word in words:\n",
    "        # 计算单词的哈希值\n",
    "        word_hash = hash(word)\n",
    "        for i in range(64):\n",
    "            # 统计每一位的哈希值的二进制位\n",
    "            feature_vector[i] += 1 if (word_hash >> i) & 1 else -1\n",
    "    # 将特征向量转换为 SimHash 值\n",
    "    simhash_value = \"\".join([\"1\" if num >= 0 else \"0\" for num in feature_vector])\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, StringType())\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"document\"]))\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 注册 Hamming Distance UDF\n",
    "spark.udf.register(\"hamming_distance_udf\", hamming_distance_udf)\n",
    "\n",
    "# 构建相似文档对的候选集\n",
    "similar_docs = df.alias(\"d1\").join(df.alias(\"d2\")) \\\n",
    "                .filter(\"d1.document_id < d2.document_id\")\\\n",
    "                .selectExpr(\"d1.document_id as document_id1\",\n",
    "                            \"d2.document_id as document_id2\",\n",
    "                            \"hamming_distance_udf(d1.simhash, d2.simhash) as hamming_distance\")\n",
    "\n",
    "# 设置阈值，筛选出相似文档对\n",
    "threshold = 3\n",
    "similar_docs = similar_docs.filter(\"hamming_distance <= {}\".format(threshold))\n",
    "\n",
    "similar_docs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72a62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
