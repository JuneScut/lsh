{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea982b1",
   "metadata": {},
   "source": [
    "### 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc92581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/21 19:16:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 创建示例数据\n",
    "data = [\n",
    "    (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "    (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "    (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "    (\"document4\", \"The lazy black cat\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "dfA = spark.read.csv(\"../dataset/SICK_train.csv\", sep=\"\t\", header=True).select(\"pair_ID\", \"sentence_A\")\n",
    "dfB = spark.read.csv(\"../dataset/SICK_train.csv\", sep=\"\t\", header=True).select(\"pair_ID\", \"sentence_B\")\n",
    "\n",
    "def preprocessDocument(document):\n",
    "    return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025f3dd",
   "metadata": {},
   "source": [
    "### 不分桶的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32c8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "4\n",
      "4\n",
      "+-------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------+--------+\n",
      "|pair_ID|sentence_A                                                                                               |sentence_B                                                                     |similarity|distance|\n",
      "+-------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------+--------+\n",
      "|3517   |The man is wildly dancing                                                                                |A woman is dancing                                                             |0.6875    |20      |\n",
      "|7782   |The shirtless man in striped shorts and sunglasses is standing near a man in a white shirt and sunglasses|The muscular white man is dancing and the man behind him is wearing green beads|0.671875  |21      |\n",
      "|8992   |A young girl in a bikini is jumping on the beach                                                         |A girl with a bikini is playing in the dunes                                   |0.84375   |10      |\n",
      "|9877   |A man is putting garlic on some bread slices                                                             |A person is bowling the ingredients to the man at the mixer                    |0.75      |16      |\n",
      "|812    |A girl from Asia, in front of a window made of bricks, looks surprised                                   |A girl is wearing a t-shirt and has her mouth open.                            |0.71875   |18      |\n",
      "|5311   |A wind instrument is being played by a girl                                                              |A girl is playing a wind instrument                                            |0.78125   |14      |\n",
      "|3022   |Three teenage girls are dancing in the room                                                              |Some teenage girls are standing still in front of the camera                   |0.734375  |17      |\n",
      "|1069   |A motorbike rider is jumping over a person on a sandy track                                              |A person is riding a motorbike in the event related to motocross               |0.734375  |17      |\n",
      "|514    |A man is sitting comfortably at a table                                                                  |A man wearing a dyed black shirt is sitting at the table and laughing          |0.828125  |11      |\n",
      "|4644   |There is no man cutting a rope with a sword                                                              |A man is cutting a rope with a sword                                           |0.796875  |13      |\n",
      "|2623   |A man is wearing a hard hat and dancing                                                                  |A man is naked and standing still                                              |0.8125    |12      |\n",
      "|9140   |A snowboarder is jumping off the snow                                                                    |There is no snowboarder jumping off the snow                                   |0.75      |16      |\n",
      "|1595   |A man is pouring liquid into a pot                                                                       |The chef is carefully pouring some oil into a pan                              |0.703125  |19      |\n",
      "|7558   |A small boy in a striped shirt is sliding out of a green tube                                            |A small shirtless boy is sliding into a green tube                             |0.765625  |15      |\n",
      "|3471   |The man is thinking                                                                                      |A man is thinking                                                              |0.859375  |9       |\n",
      "|3432   |A man is playing a violin                                                                                |A man is playing a harp                                                        |0.890625  |7       |\n",
      "|7583   |The bunch of men are not playing rugby on a muddy field                                                  |Some men are playing rugby                                                     |0.765625  |15      |\n",
      "|6607   |Some people are treading through the snow of the beautiful snowy landscape                               |Four people are standing in thick snow and the sun is rising                   |0.71875   |18      |\n",
      "|792    |A man is watering the grass and excitedly playing with a dog                                             |The dog is snapping at some droplets of water                                  |0.609375  |25      |\n",
      "|4109   |A man is eating a pizza                                                                                  |A pizza is being eaten by a man                                                |0.875     |8       |\n",
      "+-------+---------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    document = preprocessDocument(document)\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "dfA = dfA.withColumn(\"simhash_A\", simhash_udf(dfA[\"sentence_A\"]))\n",
    "dfB = dfB.withColumn(\"simhash_B\", simhash_udf(dfB[\"sentence_B\"]))\n",
    "print(dfA.rdd.getNumPartitions())\n",
    "print(dfB.rdd.getNumPartitions())\n",
    "# 改成多个分区数，对比运行时间\n",
    "dfA = dfA.repartition(4)\n",
    "dfB = dfB.repartition(4)\n",
    "print(dfA.rdd.getNumPartitions())\n",
    "print(dfB.rdd.getNumPartitions())\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 定义相似度计算函数 similarity = hamming_distance / 64\n",
    "def similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 将相似度计算函数注册为 UDF\n",
    "similarity_udf = udf(similarity, FloatType())\n",
    "\n",
    "# 计算文档之间的相似度\n",
    "similar_documents = dfA.join(dfB, 'pair_ID') \\\n",
    "    .select('pair_ID', 'sentence_A', 'sentence_B', \n",
    "            similarity_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"distance\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "# threshold = 20\n",
    "# similar_documents = similar_documents.filter(col(\"distance\") < threshold)\n",
    "\n",
    "# 打印相似文档\n",
    "similar_documents.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c582227",
   "metadata": {},
   "source": [
    "### 分桶的版本\n",
    "一共n个桶，假设规定海明距离小于k的文本为相似文本，则两个数据相似的条件为至少n-k个桶中的内容相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2135260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "+-------+--------+--------------------+\n",
      "|pair_ID|distance|matching_buckets_sum|\n",
      "+-------+--------+--------------------+\n",
      "|   2957|       6|                  11|\n",
      "|   7222|       5|                  11|\n",
      "|   7556|       3|                  13|\n",
      "|   1501|       7|                  11|\n",
      "|   4163|       6|                  12|\n",
      "|   6104|       5|                  11|\n",
      "|   7839|       5|                  11|\n",
      "|   9006|       4|                  13|\n",
      "|   2675|       2|                  14|\n",
      "|   2175|       2|                  14|\n",
      "|   1048|       5|                  11|\n",
      "|   1946|       3|                  13|\n",
      "|   3627|       6|                  11|\n",
      "|   9394|       6|                  11|\n",
      "|    857|       6|                  11|\n",
      "|   2828|       3|                  13|\n",
      "|   5248|       6|                  11|\n",
      "|   7287|       7|                  11|\n",
      "|   9497|       4|                  13|\n",
      "|    772|       9|                  11|\n",
      "+-------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col, split, concat_ws, substring, expr, when\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# # 创建示例数据\n",
    "# data = [\n",
    "#     (\"document1\", \"you could create a dataset of job descriptions and calculate the candidate overlap\"),\n",
    "#     (\"document2\", \"you could do a dataset of job descriptions or calculate the candidate overlap\"),\n",
    "#     (\"document3\", \"the document similarity is subjective and in the eyes of the client\"),\n",
    "#     (\"document4\", \"The lazy black cat\"),\n",
    "# ]\n",
    "# df = spark.createDataFrame(data, [\"document_id\", \"document\"])\n",
    "# print(df.rdd.getNumPartitions())\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "dfA = dfA.withColumn(\"simhash_A\", simhash_udf(dfA[\"sentence_A\"]))\n",
    "dfB = dfB.withColumn(\"simhash_B\", simhash_udf(dfB[\"sentence_B\"]))\n",
    "\n",
    "# 设置距离的阈值\n",
    "thres_distance = 5\n",
    "\n",
    "# 将 SimHash 值映射到桶(bucket)\n",
    "num_buckets = 16 # 桶(bucket)的数量\n",
    "\n",
    "# 使用 split 函数将数组列拆分为多个列，并计算每列应包含的元素数量\n",
    "array_len = len(dfA.first()['simhash_A'])\n",
    "elements_per_bucket = int(array_len / num_buckets)\n",
    "\n",
    "# 循环创建新的数组列，并使用 split 函数拆分源数组列中的元素\n",
    "for i in range(num_buckets):\n",
    "    # 计算拆分的起始和结束位置\n",
    "    start = i * elements_per_bucket\n",
    "    end = start + elements_per_bucket\n",
    "    # 使用 split 函数拆分数组列，并为新列命名\n",
    "    new_col_name = f\"bucket_{i}\"\n",
    "    dfA = dfA.withColumn(f\"{new_col_name}_A\", expr(f\"slice(simhash_A, {start + 1}, {end - start})\"))\n",
    "    dfB = dfB.withColumn(f\"{new_col_name}_B\", expr(f\"slice(simhash_B, {start + 1}, {end - start})\"))\n",
    "# df.show()\n",
    "\n",
    "# 要比较的buckets list\n",
    "columns_to_compare = [f\"bucket_{i}\" for i in range(num_buckets)]\n",
    "\n",
    "# 计算每两个documents之间有多少buckets相同\n",
    "matching_columns_expr = [expr(f\"CASE WHEN {col_name}_A = {col_name}_B THEN 1 ELSE 0 END as matching_buckets_{col_name}\") for col_name in columns_to_compare]\n",
    "\n",
    "# 使用 select 函数选择需要的列，并显示结果\n",
    "# df_result = df.alias(\"df1\").join(df.alias(\"df2\"), col(\"df1.document_id\") < col(\"df2.document_id\"))\\\n",
    "#     .select(\"df1.document_id\", \"df2.document_id\", *matching_columns_expr)\n",
    "# df_result.show()\n",
    "\n",
    "# 累加求和操作\n",
    "matching_columns_sum_expr = \"+\".join([f\"matching_buckets_{col_name}\" for col_name in columns_to_compare])\n",
    "# 使用 join 函数连接两个 DataFrame，并使用 groupBy 和 sum 函数进行累加求和操作\n",
    "df = dfA.join(dfB, 'pair_ID')\\\n",
    "    .select('*', *matching_columns_expr)\\\n",
    "    .groupBy(\"pair_ID\", hamming_distance_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"distance\"))\\\n",
    "    .agg(expr(f\"sum({'+'.join([f'matching_buckets_{col_name}' for col_name in columns_to_compare])}) as matching_buckets_sum\"))\n",
    "\n",
    "# 设置阈值过滤出相似的文档\n",
    "threshold = num_buckets - thres_distance\n",
    "df = df.filter(col(\"matching_buckets_sum\") >= threshold)\n",
    "\n",
    "# 显示结果\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e088aa5",
   "metadata": {},
   "source": [
    "### 图像相似度对比（不用了）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b95f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|              image1|              image2|similarity|distance|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "|[15, 19, 24, 18, ...|[107, 103, 103, 1...|  0.953125|       3|\n",
      "+--------------------+--------------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType, FloatType\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "# 加载示例图像\n",
    "image1 = Image.open(\"data/test1.png\")\n",
    "image2 = Image.open(\"data/test2.png\")\n",
    "\n",
    "# 将图像缩放为 32x32 并转换为灰度图像\n",
    "image1 = image1.resize((32, 32)).convert(\"L\")\n",
    "image2 = image2.resize((32, 32)).convert(\"L\")\n",
    "\n",
    "# 将图像数据转换为 NumPy 数组\n",
    "image1_array = np.array(image1)\n",
    "image2_array = np.array(image2)\n",
    "\n",
    "# 将图像数据展平为一维数组\n",
    "image1_vector = image1_array.flatten().tolist()\n",
    "image2_vector = image2_array.flatten().tolist()\n",
    "\n",
    "print(len(image1_vector))\n",
    "\n",
    "# 将图像数据转换为 DataFrame\n",
    "df = spark.createDataFrame([(image1_vector,), (image2_vector,)], [\"image\"])\n",
    "\n",
    "# 注册 DataFrame 为临时表\n",
    "df.createOrReplaceTempView(\"image_data\")\n",
    "\n",
    "# 将图像数据转换为 SimHash 值\n",
    "def simhash(image_vector):\n",
    "    # 将像素值转换为 SimHash 值\n",
    "    hashes = []\n",
    "    for pixel in image_vector:\n",
    "        pixel_hash = hash(pixel)\n",
    "        # 使用 64 位 SimHash 值，将像素值的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(pixel_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 对文档进行 SimHash 计算\n",
    "df = df.withColumn(\"simhash\", simhash_udf(df[\"image\"]))\n",
    "\n",
    "# 定义 Jaccard 相似度计算函数\n",
    "def jaccard_similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 计算 SimHash 值之间的 Jaccard 相似度\n",
    "similar_documents = df.alias(\"a\").join(df.alias(\"b\"), col(\"a.image\") < col(\"b.image\")) \\\n",
    "    .select(col(\"a.image\").alias(\"image1\"),\n",
    "            col(\"b.image\").alias(\"image2\"),\n",
    "            jaccard_similarity_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"similarity\"),\n",
    "            hamming_distance_udf(col(\"a.simhash\"), col(\"b.simhash\")).alias(\"distance\"))\n",
    "\n",
    "df = df.alias(\"df1\").crossJoin(df.alias(\"df2\"))\n",
    "df = df.filter(col(\"df1.image\") < col(\"df2.image\"))  # 排除与自身比较的情况\n",
    "df = df.withColumn(\"distance\", hamming_distance_udf(col(\"df1.simhash\"), col(\"df2.simhash\")))\n",
    "\n",
    "similar_documents.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
