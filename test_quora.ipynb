{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11563a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30fd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2382718",
   "metadata": {},
   "source": [
    "# minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a969ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import LongType, FloatType\n",
    "\n",
    "import random\n",
    "\n",
    "config = SparkConf().setAppName(\"LSH\").setMaster(\"local[8]\")\n",
    "sc = SparkContext.getOrCreate(config)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721ccded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SHINGLE_LENGTH = 3\n",
    "PRIME = 2147483647\n",
    "SIMILARITY_THRESHOLD = 0.001\n",
    "BAND_SIMILARITY_THRESHOLD = 0.001\n",
    "NUMBER_OF_BANDS = 10\n",
    "NUMBER_OF_ROWS = 5\n",
    "NUMBER_OF_HASHFUNCTIONS = 20\n",
    "\n",
    "class minLSH:\n",
    "    def __init__(self, numHashTables=5, shingleSize=5, inputCol=\"features\", outputCol=\"hashes\"):\n",
    "        self.numHashTables = numHashTables\n",
    "        self.shingleSize = shingleSize\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.hashFunctions = self.getHashFunctions(self.numHashTables)\n",
    "\n",
    "    def getHashFunctions(self, number):\n",
    "        result = []\n",
    "        rand = random.Random()\n",
    "        for _ in range(number):\n",
    "            a = rand.randint(0, PRIME-1)\n",
    "            b = rand.randint(0, PRIME-1)\n",
    "            result.append(lambda x, a=a, b=b: ((a * x + b) % PRIME))\n",
    "        return result\n",
    "    \n",
    "    def preprocessDocument(self, document):\n",
    "        return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \") if isinstance(document, str) else document\n",
    "\n",
    "    def shingle(self, document):\n",
    "        resultingList = []\n",
    "        i = 0\n",
    "        while i + self.shingleSize < len(document):\n",
    "            resultingList.append(hash(document[i:i+self.shingleSize]))\n",
    "            i += 1\n",
    "        return resultingList\n",
    "\n",
    "    def minHash(self, listOfShingles):\n",
    "        result = []\n",
    "        for ind in range(len(self.hashFunctions)):\n",
    "            minVal = float(\"inf\")\n",
    "            for shingle in listOfShingles:\n",
    "                hashResult = self.hashFunctions[ind](shingle)\n",
    "                if hashResult < minVal:\n",
    "                    minVal = hashResult\n",
    "            result.append(minVal)\n",
    "        return result\n",
    "    \n",
    "    def signatureToHashedBandsOfRows(self, signature, numberOfBands, numberOfRowsInBand):\n",
    "        if len(signature) != numberOfBands * numberOfRowsInBand:\n",
    "            raise Exception(\"Wrong arguments number of bands times number of rows should equal length of signature\")\n",
    "        i = 0\n",
    "        bands = []\n",
    "        while i + numberOfRowsInBand <= len(signature):\n",
    "            bands.append(signature[i:i+numberOfRowsInBand])\n",
    "            i += numberOfRowsInBand\n",
    "        return [hash(tuple(band)) for band in bands]\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        preprocessDocument_udf = udf(self.preprocessDocument, StringType())\n",
    "        df = dataframe.withColumn(\"document\", preprocessDocument_udf(self.inputCol))\n",
    "        # 分词\n",
    "        shingle_udf = udf(self.shingle, ArrayType(LongType()))\n",
    "        df = df.withColumn(\"shingles\", shingle_udf(\"document\")).drop(\"document\")\n",
    "        # MinHash\n",
    "        minHash_udf = udf(lambda x: self.minHash(x), ArrayType(LongType()))\n",
    "        df = df.withColumn(\"minHash\", minHash_udf(\"shingles\")).drop(\"shingles\")\n",
    "        return df\n",
    "\n",
    "    def approxSimilarityJoin(self, dataframeA, dataframeB, threshold, joinID=\"id\"):\n",
    "        # 相似度 udf\n",
    "        jaccard_similarity = udf(lambda x, y: len(\n",
    "                    set(x).intersection(set(y))) / len(set(x).union(set(y))), FloatType())\n",
    "\n",
    "        # 相似度\n",
    "        similarities = dataframeA.alias(\"left\").join(dataframeB.alias(\"right\"), on=joinID, how=\"inner\") \\\n",
    "            .withColumn(\"similarity\", jaccard_similarity(col(\"left.minHash\"), col(\"right.minHash\")))  \\\n",
    "            .filter(col(\"similarity\") >= threshold)\n",
    "\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ddb4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|            sentence|is_duplicate|\n",
      "+---+--------------------+------------+\n",
      "|  0|What is the step ...|           0|\n",
      "|  1|What is the story...|           0|\n",
      "|  2|How can I increas...|           0|\n",
      "|  3|Why am I mentally...|           0|\n",
      "|  4|Which one dissolv...|           0|\n",
      "+---+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+------------+\n",
      "| id|            sentence|is_duplicate|\n",
      "+---+--------------------+------------+\n",
      "|  0|What is the step ...|           0|\n",
      "|  1|What would happen...|           0|\n",
      "|  2|How can Internet ...|           0|\n",
      "|  3|Find the remainde...|           0|\n",
      "|  4|Which fish would ...|           0|\n",
      "+---+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA = spark.read.csv(\"quora/train.csv\", header=True).select(\"id\", \"question1\", \"is_duplicate\").withColumnRenamed(\"question1\", \"sentence\")\n",
    "dfB = spark.read.csv(\"quora/train.csv\", header=True).select(\"id\", \"question2\", \"is_duplicate\").withColumnRenamed(\"question2\", \"sentence\")\n",
    "\n",
    "dfA = dfA.where(dfA.is_duplicate.isin([0,1]))\n",
    "dfA = dfA.where(\"sentence is not null\")\n",
    "dfB = dfB.where(dfB.is_duplicate.isin([0,1]))\n",
    "dfB = dfB.where(\"sentence is not null\")\n",
    "\n",
    "dfA.show(5)\n",
    "dfB.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024d5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of partition: 5 ; Time: 0.23078298568725586\n",
      "Num of partition: 10 ; Time: 0.19745111465454102\n",
      "Num of partition: 20 ; Time: 0.15598821640014648\n",
      "Num of partition: 50 ; Time: 0.14315271377563477\n"
     ]
    }
   ],
   "source": [
    "# num of partition 对时间的影响\n",
    "\n",
    "for numPar in [5, 10, 20, 50]:\n",
    "    A = dfA.repartition(numPar)\n",
    "    B = dfB.repartition(numPar)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = minLSH(numHashTables=10, shingleSize=3, inputCol=\"sentence\", outputCol=\"hashes\")\n",
    "    A = model.transform(A)\n",
    "    B = model.transform(B)\n",
    "    \n",
    "    result = model.approxSimilarityJoin(A, B, 0, joinID=\"id\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    \n",
    "    print(\"Num of partition:\", numPar, \"; Time:\", run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6851e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/spark-3.3.1-bin-hadoop2/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5962023321616184"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC\n",
    "\n",
    "tmp = result.select(\"id\", \"right.is_duplicate\", \"similarity\")\n",
    "tmp = tmp.withColumn(\"is_duplicate\", tmp[\"is_duplicate\"].cast(DoubleType()))\n",
    "metrics = BinaryClassificationMetrics(tmp.select(\"is_duplicate\", \"similarity\").rdd.map(tuple))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f63e9f",
   "metadata": {},
   "source": [
    "# simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e21160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/23 09:25:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "def preprocessDocument(document):\n",
    "    return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \")\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    document = preprocessDocument(document)\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 定义相似度计算函数 similarity = hamming_distance / 64\n",
    "def similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 将相似度计算函数注册为 UDF\n",
    "similarity_udf = udf(similarity, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f825b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|           question1|is_duplicate|\n",
      "+---+--------------------+------------+\n",
      "|  0|What is the step ...|           0|\n",
      "|  1|What is the story...|           0|\n",
      "|  2|How can I increas...|           0|\n",
      "|  3|Why am I mentally...|           0|\n",
      "|  4|Which one dissolv...|           0|\n",
      "+---+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+------------+\n",
      "| id|           question2|is_duplicate|\n",
      "+---+--------------------+------------+\n",
      "|  0|What is the step ...|           0|\n",
      "|  1|What would happen...|           0|\n",
      "|  2|How can Internet ...|           0|\n",
      "|  3|Find the remainde...|           0|\n",
      "|  4|Which fish would ...|           0|\n",
      "+---+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA = spark.read.csv(\"quora/train.csv\", header=True).select(\"id\", \"question1\", \"is_duplicate\")\n",
    "dfB = spark.read.csv(\"quora/train.csv\", header=True).select(\"id\", \"question2\", \"is_duplicate\")\n",
    "\n",
    "dfA = dfA.where(dfA.is_duplicate.isin([0,1]))\n",
    "dfA = dfA.where(\"question1 is not null\")\n",
    "dfB = dfB.where(dfB.is_duplicate.isin([0,1]))\n",
    "dfB = dfB.where(\"question2 is not null\")\n",
    "\n",
    "dfA.show(5)\n",
    "dfB.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f1541a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of partition: 5 ; Time: 0.041646480560302734\n",
      "Num of partition: 10 ; Time: 0.03614950180053711\n",
      "Num of partition: 20 ; Time: 0.03628945350646973\n",
      "Num of partition: 50 ; Time: 0.04783296585083008\n"
     ]
    }
   ],
   "source": [
    "for numPar in [5, 10, 20, 50]:\n",
    "    \n",
    "    # 改成多个分区数，对比运行时间\n",
    "    dfA = dfA.repartition(numPar)\n",
    "    dfB = dfB.repartition(numPar)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 对文档进行 SimHash 计算\n",
    "    dfA = dfA.withColumn(\"simhash_A\", simhash_udf(dfA[\"question1\"]))\n",
    "    dfB = dfB.withColumn(\"simhash_B\", simhash_udf(dfB[\"question2\"]))\n",
    "\n",
    "    # 计算文档之间的相似度\n",
    "    similar_documents = dfA.join(dfB, 'id') \\\n",
    "        .select('id', 'question1', 'question2', \n",
    "                similarity_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"similarity\"),\n",
    "                hamming_distance_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"distance\"))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    \n",
    "    print(\"Num of partition:\", numPar, \"; Time:\", run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64a82ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:=========================>                                (4 + 5) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+\n",
      "|    id|similarity|is_duplicate|\n",
      "+------+----------+------------+\n",
      "|100010|   0.84375|           0|\n",
      "|100014|   0.78125|           1|\n",
      "|100021|    0.5625|           0|\n",
      "|100062|   0.59375|           0|\n",
      "|100070|    0.5625|           0|\n",
      "+------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = similar_documents.select(\"id\", \"similarity\").join(dfB.select(\"id\", \"is_duplicate\"), \"id\")\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0862abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6652052088216065"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC\n",
    "\n",
    "tmp = result.withColumn(\"is_duplicate\", result[\"is_duplicate\"].cast(DoubleType()))\n",
    "metrics = BinaryClassificationMetrics(tmp.select(\"is_duplicate\", \"similarity\").rdd.map(tuple))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755e5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
