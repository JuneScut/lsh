{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11563a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30fd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2382718",
   "metadata": {},
   "source": [
    "# minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a969ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import LongType, FloatType\n",
    "\n",
    "import random\n",
    "\n",
    "config = SparkConf().setAppName(\"LSH\").setMaster(\"local[8]\")\n",
    "sc = SparkContext.getOrCreate(config)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721ccded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SHINGLE_LENGTH = 3\n",
    "PRIME = 2147483647\n",
    "SIMILARITY_THRESHOLD = 0.001\n",
    "BAND_SIMILARITY_THRESHOLD = 0.001\n",
    "NUMBER_OF_BANDS = 10\n",
    "NUMBER_OF_ROWS = 5\n",
    "NUMBER_OF_HASHFUNCTIONS = 20\n",
    "\n",
    "class minLSH:\n",
    "    def __init__(self, numHashTables=5, shingleSize=5, inputCol=\"features\", outputCol=\"hashes\"):\n",
    "        self.numHashTables = numHashTables\n",
    "        self.shingleSize = shingleSize\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.hashFunctions = self.getHashFunctions(self.numHashTables)\n",
    "\n",
    "    def getHashFunctions(self, number):\n",
    "        result = []\n",
    "        rand = random.Random()\n",
    "        for _ in range(number):\n",
    "            a = rand.randint(0, PRIME-1)\n",
    "            b = rand.randint(0, PRIME-1)\n",
    "            result.append(lambda x, a=a, b=b: ((a * x + b) % PRIME))\n",
    "        return result\n",
    "    \n",
    "    def preprocessDocument(self, document):\n",
    "        return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \") if isinstance(document, str) else document\n",
    "\n",
    "    def shingle(self, document):\n",
    "        resultingList = []\n",
    "        i = 0\n",
    "        while i + self.shingleSize < len(document):\n",
    "            resultingList.append(hash(document[i:i+self.shingleSize]))\n",
    "            i += 1\n",
    "        return resultingList\n",
    "\n",
    "    def minHash(self, listOfShingles):\n",
    "        result = []\n",
    "        for ind in range(len(self.hashFunctions)):\n",
    "            minVal = float(\"inf\")\n",
    "            for shingle in listOfShingles:\n",
    "                hashResult = self.hashFunctions[ind](shingle)\n",
    "                if hashResult < minVal:\n",
    "                    minVal = hashResult\n",
    "            result.append(minVal)\n",
    "        return result\n",
    "    \n",
    "    def signatureToHashedBandsOfRows(self, signature, numberOfBands, numberOfRowsInBand):\n",
    "        if len(signature) != numberOfBands * numberOfRowsInBand:\n",
    "            raise Exception(\"Wrong arguments number of bands times number of rows should equal length of signature\")\n",
    "        i = 0\n",
    "        bands = []\n",
    "        while i + numberOfRowsInBand <= len(signature):\n",
    "            bands.append(signature[i:i+numberOfRowsInBand])\n",
    "            i += numberOfRowsInBand\n",
    "        return [hash(tuple(band)) for band in bands]\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        preprocessDocument_udf = udf(self.preprocessDocument, StringType())\n",
    "        df = dataframe.withColumn(\"document\", preprocessDocument_udf(self.inputCol))\n",
    "        # 分词\n",
    "        shingle_udf = udf(self.shingle, ArrayType(LongType()))\n",
    "        df = df.withColumn(\"shingles\", shingle_udf(\"document\")).drop(\"document\")\n",
    "        # MinHash\n",
    "        minHash_udf = udf(lambda x: self.minHash(x), ArrayType(LongType()))\n",
    "        df = df.withColumn(\"minHash\", minHash_udf(\"shingles\")).drop(\"shingles\")\n",
    "        return df\n",
    "\n",
    "    def approxSimilarityJoin(self, dataframeA, dataframeB, threshold, joinID=\"id\"):\n",
    "        # 相似度 udf\n",
    "        jaccard_similarity = udf(lambda x, y: len(\n",
    "                    set(x).intersection(set(y))) / len(set(x).union(set(y))), FloatType())\n",
    "\n",
    "        # 相似度\n",
    "        similarities = dataframeA.alias(\"left\").join(dataframeB.alias(\"right\"), on=joinID, how=\"inner\") \\\n",
    "            .withColumn(\"similarity\", jaccard_similarity(col(\"left.minHash\"), col(\"right.minHash\")))  \\\n",
    "            .filter(col(\"similarity\") >= threshold)\n",
    "\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ddb4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|            sentence|is_duplicate|\n",
      "+---+--------------------+------------+\n",
      "|  0|What is the step ...|           0|\n",
      "|  1|What is the story...|           0|\n",
      "|  2|How can I increas...|           0|\n",
      "|  3|Why am I mentally...|           0|\n",
      "|  4|Which one dissolv...|           0|\n",
      "+---+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+------------+\n",
      "| id|            sentence|is_duplicate|\n",
      "+---+--------------------+------------+\n",
      "|  0|What is the step ...|           0|\n",
      "|  1|What would happen...|           0|\n",
      "|  2|How can Internet ...|           0|\n",
      "|  3|Find the remainde...|           0|\n",
      "|  4|Which fish would ...|           0|\n",
      "+---+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA = spark.read.csv(\"quora/train.csv\", header=True).select(\"id\", \"question1\", \"is_duplicate\").withColumnRenamed(\"question1\", \"sentence\")\n",
    "dfB = spark.read.csv(\"quora/train.csv\", header=True).select(\"id\", \"question2\", \"is_duplicate\").withColumnRenamed(\"question2\", \"sentence\")\n",
    "\n",
    "dfA = dfA.where(dfA.is_duplicate.isin([0,1]))\n",
    "dfA = dfA.where(\"sentence is not null\")\n",
    "dfB = dfB.where(dfB.is_duplicate.isin([0,1]))\n",
    "dfB = dfB.where(\"sentence is not null\")\n",
    "\n",
    "dfA.show(5)\n",
    "dfB.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024d5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of partition: 5 ; Time: 0.09009051322937012\n",
      "Num of partition: 10 ; Time: 0.0484774112701416\n",
      "Num of partition: 20 ; Time: 0.045918941497802734\n",
      "Num of partition: 50 ; Time: 0.04009652137756348\n"
     ]
    }
   ],
   "source": [
    "# num of partition 对时间的影响\n",
    "\n",
    "for numPar in [5, 10, 20, 50]:\n",
    "    A = dfA.repartition(numPar)\n",
    "    B = dfB.repartition(numPar)\n",
    "\n",
    "    model = minLSH(numHashTables=10, shingleSize=3, inputCol=\"sentence\", outputCol=\"hashes\")\n",
    "    A = model.transform(A)\n",
    "    B = model.transform(B)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = model.approxSimilarityJoin(A, B, 0, joinID=\"id\")\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    \n",
    "    print(\"Num of partition:\", numPar, \"; Time:\", run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6851e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/spark-3.3.1-bin-hadoop2/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5993976139392746"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全样本的AUC\n",
    "\n",
    "tmp = result.select(\"id\", \"right.is_duplicate\", \"similarity\")\n",
    "tmp = tmp.withColumn(\"is_duplicate\", tmp[\"is_duplicate\"].cast(DoubleType()))\n",
    "metrics = BinaryClassificationMetrics(tmp.select(\"is_duplicate\", \"similarity\").rdd.map(tuple))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc7bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
