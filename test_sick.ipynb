{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef75b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b748fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79090453",
   "metadata": {},
   "source": [
    "# minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06aeecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import LongType, FloatType\n",
    "\n",
    "import random\n",
    "\n",
    "config = SparkConf().setAppName(\"LSH\").setMaster(\"local[8]\")\n",
    "sc = SparkContext.getOrCreate(config)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3909fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SHINGLE_LENGTH = 3\n",
    "PRIME = 2147483647\n",
    "SIMILARITY_THRESHOLD = 0.001\n",
    "BAND_SIMILARITY_THRESHOLD = 0.001\n",
    "NUMBER_OF_BANDS = 10\n",
    "NUMBER_OF_ROWS = 5\n",
    "NUMBER_OF_HASHFUNCTIONS = 20\n",
    "\n",
    "class minLSH:\n",
    "    def __init__(self, numHashTables=5, shingleSize=5, inputCol=\"features\", outputCol=\"hashes\"):\n",
    "        self.numHashTables = numHashTables\n",
    "        self.shingleSize = shingleSize\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.hashFunctions = self.getHashFunctions(self.numHashTables)\n",
    "\n",
    "    def getHashFunctions(self, number):\n",
    "        result = []\n",
    "        rand = random.Random()\n",
    "        for _ in range(number):\n",
    "            a = rand.randint(0, PRIME-1)\n",
    "            b = rand.randint(0, PRIME-1)\n",
    "            result.append(lambda x, a=a, b=b: ((a * x + b) % PRIME))\n",
    "        return result\n",
    "    \n",
    "    def preprocessDocument(self, document):\n",
    "        return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \") if isinstance(document, str) else document\n",
    "\n",
    "    def shingle(self, document):\n",
    "        resultingList = []\n",
    "        i = 0\n",
    "        while i + self.shingleSize < len(document):\n",
    "            resultingList.append(hash(document[i:i+self.shingleSize]))\n",
    "            i += 1\n",
    "        return resultingList\n",
    "\n",
    "    def minHash(self, listOfShingles):\n",
    "        result = []\n",
    "        for ind in range(len(self.hashFunctions)):\n",
    "            minVal = float(\"inf\")\n",
    "            for shingle in listOfShingles:\n",
    "                hashResult = self.hashFunctions[ind](shingle)\n",
    "                if hashResult < minVal:\n",
    "                    minVal = hashResult\n",
    "            result.append(minVal)\n",
    "        return result\n",
    "    \n",
    "    def signatureToHashedBandsOfRows(self, signature, numberOfBands, numberOfRowsInBand):\n",
    "        if len(signature) != numberOfBands * numberOfRowsInBand:\n",
    "            raise Exception(\"Wrong arguments number of bands times number of rows should equal length of signature\")\n",
    "        i = 0\n",
    "        bands = []\n",
    "        while i + numberOfRowsInBand <= len(signature):\n",
    "            bands.append(signature[i:i+numberOfRowsInBand])\n",
    "            i += numberOfRowsInBand\n",
    "        return [hash(tuple(band)) for band in bands]\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        preprocessDocument_udf = udf(self.preprocessDocument, StringType())\n",
    "        df = dataframe.withColumn(\"document\", preprocessDocument_udf(self.inputCol))\n",
    "        # 分词\n",
    "        shingle_udf = udf(self.shingle, ArrayType(LongType()))\n",
    "        df = df.withColumn(\"shingles\", shingle_udf(\"document\")).drop(\"document\")\n",
    "        # MinHash\n",
    "        minHash_udf = udf(lambda x: self.minHash(x), ArrayType(LongType()))\n",
    "        df = df.withColumn(\"minHash\", minHash_udf(\"shingles\")).drop(\"shingles\")\n",
    "        return df\n",
    "\n",
    "    def approxSimilarityJoin(self, dataframeA, dataframeB, threshold, joinID=\"id\"):\n",
    "        # 相似度 udf\n",
    "        jaccard_similarity = udf(lambda x, y: len(\n",
    "                    set(x).intersection(set(y))) / len(set(x).union(set(y))), FloatType())\n",
    "\n",
    "        # 相似度\n",
    "        similarities = dataframeA.alias(\"left\").join(dataframeB.alias(\"right\"), on=joinID, how=\"inner\") \\\n",
    "            .withColumn(\"similarity\", jaccard_similarity(col(\"left.minHash\"), col(\"right.minHash\")))  \\\n",
    "            .filter(col(\"similarity\") >= threshold)\n",
    "\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ce4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_threshold = udf(lambda x: 1 if x>=3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14985fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  1|A group of kids i...|\n",
      "|  2|A group of childr...|\n",
      "|  3|The young boys ar...|\n",
      "|  5|The kids are play...|\n",
      "|  9|The young boys ar...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+-----------------+------------+\n",
      "| id|            sentence|relatedness_score|is_duplicate|\n",
      "+---+--------------------+-----------------+------------+\n",
      "|  1|A group of boys i...|              4.5|           1|\n",
      "|  2|A group of kids i...|              3.2|           1|\n",
      "|  3|The kids are play...|              4.7|           1|\n",
      "|  5|A group of kids i...|              3.4|           1|\n",
      "|  9|A group of kids i...|              3.7|           1|\n",
      "+---+--------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA = spark.read.csv(\"sick/train.csv\", header=True).select(\"pair_ID\", \"sentence_A\").withColumnRenamed(\"sentence_A\", \"sentence\").withColumnRenamed(\"pair_ID\", \"id\")\n",
    "dfB = spark.read.csv(\"sick/train.csv\", header=True).select(\"pair_ID\", \"sentence_B\", \"relatedness_score\").withColumnRenamed(\"sentence_B\", \"sentence\").withColumnRenamed(\"pair_ID\", \"id\")\n",
    "\n",
    "dfB = dfB.withColumn(\"relatedness_score\", dfB[\"relatedness_score\"].cast(DoubleType()))\n",
    "dfB = dfB.withColumn(\"is_duplicate\", manual_threshold(\"relatedness_score\"))\n",
    "\n",
    "dfA.show(5)\n",
    "dfB.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d97d238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of partition: 5 ; Time: 0.07942008972167969\n",
      "Num of partition: 10 ; Time: 0.061505794525146484\n",
      "Num of partition: 20 ; Time: 0.07738327980041504\n",
      "Num of partition: 50 ; Time: 0.04362154006958008\n"
     ]
    }
   ],
   "source": [
    "# num of partition 对时间的影响\n",
    "\n",
    "for numPar in [5, 10, 20, 50]:\n",
    "    A = dfA.repartition(numPar)\n",
    "    B = dfB.repartition(numPar)\n",
    "\n",
    "    model = minLSH(numHashTables=10, shingleSize=3, inputCol=\"sentence\", outputCol=\"hashes\")\n",
    "    A = model.transform(A)\n",
    "    B = model.transform(B)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = model.approxSimilarityJoin(A, B, 0, joinID=\"id\")\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    \n",
    "    print(\"Num of partition:\", numPar, \"; Time:\", run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f8345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/spark-3.3.1-bin-hadoop2/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6438155987559453"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train set 的AUC\n",
    "\n",
    "tmp = result.select(\"id\", \"is_duplicate\", \"similarity\")\n",
    "tmp = tmp.withColumn(\"is_duplicate\", tmp[\"is_duplicate\"].cast(DoubleType()))\n",
    "metrics = BinaryClassificationMetrics(tmp.select(\"is_duplicate\", \"similarity\").rdd.map(tuple))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88c887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
