{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07108f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7eca5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fb527",
   "metadata": {},
   "source": [
    "# minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760633a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import LongType, FloatType\n",
    "\n",
    "import random\n",
    "\n",
    "config = SparkConf().setAppName(\"LSH\").setMaster(\"local[8]\")\n",
    "sc = SparkContext.getOrCreate(config)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce8d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SHINGLE_LENGTH = 3\n",
    "PRIME = 2147483647\n",
    "SIMILARITY_THRESHOLD = 0.001\n",
    "BAND_SIMILARITY_THRESHOLD = 0.001\n",
    "NUMBER_OF_BANDS = 10\n",
    "NUMBER_OF_ROWS = 5\n",
    "NUMBER_OF_HASHFUNCTIONS = 20\n",
    "\n",
    "class minLSH:\n",
    "    def __init__(self, numHashTables=5, shingleSize=5, inputCol=\"features\", outputCol=\"hashes\"):\n",
    "        self.numHashTables = numHashTables\n",
    "        self.shingleSize = shingleSize\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        self.hashFunctions = self.getHashFunctions(self.numHashTables)\n",
    "\n",
    "    def getHashFunctions(self, number):\n",
    "        result = []\n",
    "        rand = random.Random()\n",
    "        for _ in range(number):\n",
    "            a = rand.randint(0, PRIME-1)\n",
    "            b = rand.randint(0, PRIME-1)\n",
    "            result.append(lambda x, a=a, b=b: ((a * x + b) % PRIME))\n",
    "        return result\n",
    "    \n",
    "    def preprocessDocument(self, document):\n",
    "        return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \") if isinstance(document, str) else document\n",
    "\n",
    "    def shingle(self, document):\n",
    "        resultingList = []\n",
    "        i = 0\n",
    "        while i + self.shingleSize < len(document):\n",
    "            resultingList.append(hash(document[i:i+self.shingleSize]))\n",
    "            i += 1\n",
    "        return resultingList\n",
    "\n",
    "    def minHash(self, listOfShingles):\n",
    "        result = []\n",
    "        for ind in range(len(self.hashFunctions)):\n",
    "            minVal = float(\"inf\")\n",
    "            for shingle in listOfShingles:\n",
    "                hashResult = self.hashFunctions[ind](shingle)\n",
    "                if hashResult < minVal:\n",
    "                    minVal = hashResult\n",
    "            result.append(minVal)\n",
    "        return result\n",
    "    \n",
    "    def signatureToHashedBandsOfRows(self, signature, numberOfBands, numberOfRowsInBand):\n",
    "        if len(signature) != numberOfBands * numberOfRowsInBand:\n",
    "            raise Exception(\"Wrong arguments number of bands times number of rows should equal length of signature\")\n",
    "        i = 0\n",
    "        bands = []\n",
    "        while i + numberOfRowsInBand <= len(signature):\n",
    "            bands.append(signature[i:i+numberOfRowsInBand])\n",
    "            i += numberOfRowsInBand\n",
    "        return [hash(tuple(band)) for band in bands]\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        preprocessDocument_udf = udf(self.preprocessDocument, StringType())\n",
    "        df = dataframe.withColumn(\"document\", preprocessDocument_udf(self.inputCol))\n",
    "        # 分词\n",
    "        shingle_udf = udf(self.shingle, ArrayType(LongType()))\n",
    "        df = df.withColumn(\"shingles\", shingle_udf(\"document\")).drop(\"document\")\n",
    "        # MinHash\n",
    "        minHash_udf = udf(lambda x: self.minHash(x), ArrayType(LongType()))\n",
    "        df = df.withColumn(\"minHash\", minHash_udf(\"shingles\")).drop(\"shingles\")\n",
    "        return df\n",
    "\n",
    "    def approxSimilarityJoin(self, dataframeA, dataframeB, threshold, joinID=\"id\"):\n",
    "        # 相似度 udf\n",
    "        jaccard_similarity = udf(lambda x, y: len(\n",
    "                    set(x).intersection(set(y))) / len(set(x).union(set(y))), FloatType())\n",
    "\n",
    "        # 相似度\n",
    "        similarities = dataframeA.alias(\"left\").join(dataframeB.alias(\"right\"), on=joinID, how=\"inner\") \\\n",
    "            .withColumn(\"similarity\", jaccard_similarity(col(\"left.minHash\"), col(\"right.minHash\")))  \\\n",
    "            .filter(col(\"similarity\") >= threshold)\n",
    "\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c4a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_threshold = udf(lambda x: 1 if x>=3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c025e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  1|A group of kids i...|\n",
      "|  2|A group of childr...|\n",
      "|  3|The young boys ar...|\n",
      "|  5|The kids are play...|\n",
      "|  9|The young boys ar...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+-----------------+------------+\n",
      "| id|            sentence|relatedness_score|is_duplicate|\n",
      "+---+--------------------+-----------------+------------+\n",
      "|  1|A group of boys i...|              4.5|           1|\n",
      "|  2|A group of kids i...|              3.2|           1|\n",
      "|  3|The kids are play...|              4.7|           1|\n",
      "|  5|A group of kids i...|              3.4|           1|\n",
      "|  9|A group of kids i...|              3.7|           1|\n",
      "+---+--------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfA = spark.read.csv(\"sick/train.csv\", header=True).select(\"pair_ID\", \"sentence_A\").withColumnRenamed(\"sentence_A\", \"sentence\").withColumnRenamed(\"pair_ID\", \"id\")\n",
    "dfB = spark.read.csv(\"sick/train.csv\", header=True).select(\"pair_ID\", \"sentence_B\", \"relatedness_score\").withColumnRenamed(\"sentence_B\", \"sentence\").withColumnRenamed(\"pair_ID\", \"id\")\n",
    "\n",
    "dfB = dfB.withColumn(\"relatedness_score\", dfB[\"relatedness_score\"].cast(DoubleType()))\n",
    "dfB = dfB.withColumn(\"is_duplicate\", manual_threshold(\"relatedness_score\"))\n",
    "\n",
    "dfA.show(5)\n",
    "dfB.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3829254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of partition: 5 ; Time: 0.3506152629852295\n",
      "Num of partition: 10 ; Time: 0.31075048446655273\n",
      "Num of partition: 20 ; Time: 0.3353879451751709\n",
      "Num of partition: 50 ; Time: 0.3309915065765381\n"
     ]
    }
   ],
   "source": [
    "# num of partition 对时间的影响\n",
    "\n",
    "for numPar in [5, 10, 20, 50]:\n",
    "    A = dfA.repartition(numPar)\n",
    "    B = dfB.repartition(numPar)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = minLSH(numHashTables=10, shingleSize=3, inputCol=\"sentence\", outputCol=\"hashes\")\n",
    "    A = model.transform(A)\n",
    "    B = model.transform(B)\n",
    "    \n",
    "    result = model.approxSimilarityJoin(A, B, 0, joinID=\"id\")\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    \n",
    "    print(\"Num of partition:\", numPar, \"; Time:\", run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3746ff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy/spark-3.3.1-bin-hadoop2/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6438155987559453"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC\n",
    "\n",
    "tmp = result.select(\"id\", \"is_duplicate\", \"similarity\")\n",
    "tmp = tmp.withColumn(\"is_duplicate\", tmp[\"is_duplicate\"].cast(DoubleType()))\n",
    "metrics = BinaryClassificationMetrics(tmp.select(\"is_duplicate\", \"similarity\").rdd.map(tuple))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962ce91",
   "metadata": {},
   "source": [
    "# simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55dd053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SimHashLSH\").getOrCreate()\n",
    "\n",
    "def preprocessDocument(document):\n",
    "    return document.strip().lower().replace(\"[^\\\\w\\\\s]\", \"\").replace(\"\\\\s+\", \" \")\n",
    "\n",
    "# 定义 SimHash 函数\n",
    "def simhash(document):\n",
    "    document = preprocessDocument(document)\n",
    "    # 将文档拆分为单词\n",
    "    words = document.split(\" \")\n",
    "\n",
    "    # 计算每个单词的 SimHash 值\n",
    "    hashes = []\n",
    "    for word in words:\n",
    "        word_hash = hash(word)\n",
    "        # 使用 64 位 SimHash 值，将每个单词的哈希值转换为二进制表示，并填充到 64 位\n",
    "        binary_hash = format(word_hash, \"064b\")\n",
    "        # 将二进制表示的哈希值转换为 DenseVector，每个元素值为 -1 或 1\n",
    "        vector = [1 if b == \"1\" else -1 for b in binary_hash]\n",
    "        hashes.append(vector)\n",
    "    # 将所有单词的 SimHash 值合并到一个数组中\n",
    "    simhash_value = [0] * 64\n",
    "    for v in hashes:\n",
    "        for i in range(64):\n",
    "            simhash_value[i] += v[i]\n",
    "    simhash_value = [1 if x > 0 else 0 for x in simhash_value]\n",
    "    return simhash_value\n",
    "\n",
    "# 将 SimHash 函数注册为 UDF\n",
    "simhash_udf = udf(simhash, ArrayType(IntegerType()))\n",
    "\n",
    "# 定义 Hamming Distance 函数\n",
    "def hamming_distance(simhash1, simhash2):\n",
    "    # 计算两个 SimHash 值的 Hamming 距离\n",
    "    distance = sum([1 for i in range(64) if simhash1[i] != simhash2[i]])\n",
    "    return distance\n",
    "\n",
    "# 将 Hamming Distance 函数注册为 UDF\n",
    "hamming_distance_udf = udf(hamming_distance, IntegerType())\n",
    "\n",
    "# 定义相似度计算函数 similarity = hamming_distance / 64\n",
    "def similarity(simhash1, simhash2):\n",
    "    length = len(simhash1)\n",
    "    cnt = sum([1 for i in range(length) if simhash1[i] == simhash2[i]])\n",
    "    return cnt / length\n",
    "\n",
    "# 将相似度计算函数注册为 UDF\n",
    "similarity_udf = udf(similarity, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b4c6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|pair_ID|          sentence_A|\n",
      "+-------+--------------------+\n",
      "|      1|A group of kids i...|\n",
      "|      2|A group of childr...|\n",
      "|      3|The young boys ar...|\n",
      "|      5|The kids are play...|\n",
      "|      9|The young boys ar...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------------+-----------------+------------+\n",
      "|pair_ID|          sentence_B|relatedness_score|is_duplicate|\n",
      "+-------+--------------------+-----------------+------------+\n",
      "|      1|A group of boys i...|              4.5|           1|\n",
      "|      2|A group of kids i...|              3.2|           1|\n",
      "|      3|The kids are play...|              4.7|           1|\n",
      "|      5|A group of kids i...|              3.4|           1|\n",
      "|      9|A group of kids i...|              3.7|           1|\n",
      "+-------+--------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建示例数据\n",
    "dfA = spark.read.csv(\"sick/train.csv\", header=True).select(\"pair_ID\", \"sentence_A\")\n",
    "dfB = spark.read.csv(\"sick/train.csv\", header=True).select(\"pair_ID\", \"sentence_B\", \"relatedness_score\")\n",
    "\n",
    "dfB = dfB.withColumn(\"relatedness_score\", dfB[\"relatedness_score\"].cast(DoubleType()))\n",
    "dfB = dfB.withColumn(\"is_duplicate\", manual_threshold(\"relatedness_score\"))\n",
    "\n",
    "dfA.show(5)\n",
    "dfB.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b172d41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of partition: 5 ; Time: 0.05202341079711914\n",
      "Num of partition: 10 ; Time: 0.04153585433959961\n",
      "Num of partition: 20 ; Time: 0.04148983955383301\n",
      "Num of partition: 50 ; Time: 0.039687395095825195\n"
     ]
    }
   ],
   "source": [
    "for numPar in [5, 10, 20, 50]:\n",
    "    \n",
    "    # 改成多个分区数，对比运行时间\n",
    "    dfA = dfA.repartition(numPar)\n",
    "    dfB = dfB.repartition(numPar)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 对文档进行 SimHash 计算\n",
    "    dfA = dfA.withColumn(\"simhash_A\", simhash_udf(dfA[\"sentence_A\"]))\n",
    "    dfB = dfB.withColumn(\"simhash_B\", simhash_udf(dfB[\"sentence_B\"]))\n",
    "\n",
    "    # 计算文档之间的相似度\n",
    "    similar_documents = dfA.join(dfB, 'pair_ID') \\\n",
    "        .select('pair_ID', 'sentence_A', 'sentence_B', \n",
    "                similarity_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"similarity\"),\n",
    "                hamming_distance_udf(col(\"simhash_A\"), col(\"simhash_B\")).alias(\"distance\"))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    \n",
    "    print(\"Num of partition:\", numPar, \"; Time:\", run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48cdd25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+\n",
      "|pair_ID|similarity|is_duplicate|\n",
      "+-------+----------+------------+\n",
      "|   3491|  0.609375|           0|\n",
      "|   8288|      0.75|           1|\n",
      "|   2025|  0.515625|           0|\n",
      "|   9570|    0.6875|           0|\n",
      "|   7008|  0.890625|           1|\n",
      "+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = similar_documents.select(\"pair_ID\", \"similarity\").join(dfB.select(\"pair_ID\", \"is_duplicate\"), \"pair_ID\")\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ad4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6932290157442287"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC\n",
    "\n",
    "tmp = result.withColumn(\"is_duplicate\", result[\"is_duplicate\"].cast(DoubleType()))\n",
    "metrics = BinaryClassificationMetrics(tmp.select(\"is_duplicate\", \"similarity\").rdd.map(tuple))\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74179064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
